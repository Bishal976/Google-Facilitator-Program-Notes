commands:

*> gcloud --> The main Google Cloud toolkit, which is used for many tasks on the platform, such as resource management and user authentication.

1> gcloud auth list --> auth listâ€”which lists the credentialed accounts in your Google Cloud project

2> gcloud config list project --> lists the project ids.

3> on ssh terminal:
	a> sudo su -  --> for getting root access 
	b> apt-get update   --> get update for your os
	c> apt-get install nginx -y   --> installing NGINX.(high performance load balancer and web server)
	d> ps auwx | grep nginx   --> confirm that nginx is running.

4> creating new instances with gcloud:
	a> gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone us-central1-c   --> to create a new virtual machine instance from the cmd.
	b> gcloud compute instances create --help   --> To see all the defaults for the instances.
	c> You can also use SSH to connect to your instance via gcloud. Make sure to add your zone, or omit the --zone flag if you've set the option globally:
	gcloud compute ssh gcelab2 --zone us-central1-c  

5> RDP into the windows server:
	a> gcloud compute instances get-serial-port-output instance-1    --> To see weather the server instance is redy for and RDP connection.
	b> gcloud compute reset-windows-password [instance] --zone us-central1-a --user [username]  --> setting password for logging into RDP.
	c> download the rdp and run on you system

6> Getting started with google cloud shell and gcloud:
	a> gcloud compute project-info describe --project [project_id]   --> know the default region and zone of the project 
	b> export PROJECT_ID=[your project id]   --> setting the environment variables
	c> export ZONE=<your_zone>
	d> gcloud compute instances create gcelab2 --machine-type n1-standard-2 --zone $ZONE    --> creating vm instance from console
	e> gcloud compute instances create --help   --> To open help for the create command, run the following command
	f> gcloud -h   --> The gcloud tool offers simple usage guidelines that are available by adding the -h flag (for help) onto the end of any gcloud command.
	g> gcloud config --help  ||  gcloud help config  --> You can access more verbose help by appending the --help flag onto a command or running the gcloud help command.  Note: Press ENTER or the spacebar to scroll through the help content. To exit the content, type Q.
	i> gcloud config list   --> View the list of configurations in your environment:
	j> gcloud config list --all    --> To see all properties and their settings:
	k> gcloud components list    --> List your components
--------- installing a new component --------------------------------------------- (installing auto complete mode)----------------------
	l> sudo apt-get install google-cloud-sdk     --> installing beta components
	m> gcloud beta interactive      --> enabling google cloud interractive mode.
	n> exit   --> to exit the interactive mode.
--------- connecting to your vm instance with ssh---------------------------------(firstly you can easily click on ssh under vm to connect directly you vm with ssh, else using gcloud console provides  a wrapper around SSH, which takes care of authentication and the mapping of instance names to IP addresses.)
	o> gcloud compute ssh gcelab2 --zone $ZONE   --> to connect your vm with ssh via gcloud console.
--------- using the home directory-------------------------------------------------(The contents of your Cloud Shell Home directory persist across projects between all Cloud Shell sessions, even after the virtual machine is terminated and restarted.)-------------------------------------------------------
	p> cd $HOME   --> change your current working directory to home.
	q> vi ./.bashrc    --> vi text editor opens the .bashrc files.
	r> To exit the editor, press ESC, :wq, and then press Enter.

7> Kubernetes Qwik start:
--------- setting default compute zone------------
	a> gcloud config set compute/zone us-central1-a  
--------- creating a GKE cluster------------------
	b> gcloud container clusters create [CLUSTER-NAME]   --> creates a gcloud cluster
	c> gcloud container clusters get-credentials [CLUSTER-NAME]   			--> gets authentication credentials for the cluster
--------- deploying an application to the cluster --------------------------
	d> kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0   --> creates a new Deployment hello-server from the hello-app container image,  (This Kubernetes command creates a Deployment object that represents hello-server. In this case, --image specifies a container image to deploy. The command pulls the example image from a Container Registry bucket. gcr.io/google-samples/hello-app:1.0 indicates the specific image version to pull. If a version is not specified, the latest version is used.)
	e> kubectl expose deployment hello-server --type=LoadBalancer --port 8080   --> create a Kubernetes Service, which is a Kubernetes resource that lets you expose your application to external traffic(--port specifies the port that the container exposes. type="LoadBalancer" creates a Compute Engine load balancer for your container.)
	f> kubectl get service   ||  http://[EXTERNAL-IP]:8080			--> inspect the hello-server Service
	g> gcloud container clusters delete [CLUSTER-NAME]  			--> deleting the cluster.	
8> Back Set Up Network and HTTP Load Balancers:
	a> gcloud compute instances create www1 \
  --image-family debian-9 \
  --image-project debian-cloud \
  --zone us-central1-a \
  --tags network-lb-tag \
  --metadata startup-script="#! /bin/bash
    sudo apt-get update
    sudo apt-get install apache2 -y
    sudo service apache2 restart
    echo '<!doctype html><html><body><h1>Hello World!</h1></body></html>' | sudo tee /var/www/html/index.html      ==> creating instances
	b> gcloud compute firewall-rules create www-firewall-network-lb \
    --target-tags network-lb-tag --allow tcp:80      --> creating firewall traffic to the vm instances.
	c> gcloud compute instances list   --> getting the details of the instances created along with their external and internal ip's.
	d> curl http://[IP_ADDRESS]     --> sending request to check the vm's working.
-------- configure the load balancing service -------------------------------------
	e> gcloud compute addresses create network-lb-ip-1 \
 --region us-central1                        --> creating a static external ip address for the load balancer.
	f> gcloud compute http-health-checks create basic-check           --> adding http health check resource
	g> gcloud compute target-pools create www-pool \
    --region us-central1 --http-health-check basic-check             --> Add a target pool in the same region as your instances. Run the following to create the target pool and use the health check, which is required for the service to function
	h> gcloud compute target-pools add-instances www-pool \
    --instances www1,www2,www3                                       --> adding instances to the pool
	i> gcloud compute forwarding-rules create www-rule \
    --region us-central1 \
    --ports 80 \
    --address network-lb-ip-1 \
    --target-pool www-pool                                            --> adding a forward rule 
	j> gcloud compute forwarding-rules describe www-rule --region us-central1            --> views the external ip address of the pool
	i> while true; do curl -m1 IP_ADDRESS; done                    --> sending the requests using tcp protocol.(use ctrl+c for exit).

--------- creating an http load balancer ----------------------------------------
	j> gcloud compute instance-templates create lb-backend-template \
   --region=us-central1 \
   --network=default \
   --subnet=default \
   --tags=allow-health-check \
   --image-family=debian-9 \
   --image-project=debian-cloud \
   --metadata=startup-script='#! /bin/bash
     apt-get update
     apt-get install apache2 -y
     a2ensite default-ssl
     a2enmod ssl
     vm_hostname="$(curl -H "Metadata-Flavor:Google" \
     http://169.254.169.254/computeMetadata/v1/instance/name)"
     x1                                            -->creating a load balancer template
	k> gcloud compute instance-groups managed create lb-backend-group \
   --template=lb-backend-template --size=2 --zone=us-central1-a            --> creating a managed instance grp based on the template
	l> gcloud compute firewall-rules create fw-allow-health-check \
    --network=default \

    --action=allow \
    --direction=ingress \
    --source-ranges=130.211.0.0/22,35.191.0.0/16 \
    --target-tags=allow-health-check \
    --rules=tcp:80                                                         --> allows traffic from google cloud health checking systems.
	m> gcloud compute addresses create lb-ipv4-1 \
    --ip-version=IPV4 \
    --global                                                               --> setting an global external ip address 
	n> gcloud compute addresses describe lb-ipv4-1 \
    --format="get(address)" \
    --global                                                               --> to check the external ip address alloted.
	o>     gcloud compute health-checks create http http-basic-check \
        --port 80  							   --> creating health checker for the laod balancer.
	p>     gcloud compute backend-services create web-backend-service \
        --protocol=HTTP \
        --port-name=http \
        --health-checks=http-basic-check \
        --global        						  --> creating a backend service
	q>     gcloud compute backend-services add-backend web-backend-service \
        --instance-group=lb-backend-group \
        --instance-group-zone=us-central1-a \
        --global                                                           --> adding the instance grp as backend to the backend service
	r>     gcloud compute url-maps create web-map-http \
        --default-service web-backend-service                              --> creating an URL map to route the incoming requests to the default backend service
	s>     gcloud compute target-http-proxies create http-lb-proxy \
        --url-map web-map-http                                             --> Creating a target HTTP proxy to route requests to your URL map
	t>     gcloud compute forwarding-rules create http-content-rule \
        --address=lb-ipv4-1\
        --global \
        --target-http-proxy=http-lb-proxy \
        --ports=80							    --> creating a global forwarding rule to route the incoming request to the proxy

9> Cloud Storage: Qwik Start - CLI/SDK:
	a> gsutil mb gs://YOUR BUCKET NAME/                      --> creates a bucket from the google cloud shell
	b> wget --output-document ada.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Ada_Lovelace_portrait.jpg/800px-Ada_Lovelace_portrait.jpg                --> creates an instance named ada.jpg in shell and downloads the image into it
	c> gsutil cp ada.jpg gs://YOUR BUCKET NAME               --> copies the instance to the bucket 
	d> rm ada.jpg                                            --> delets the instance ada.jpg
	e> gsutil cp -r gs://YOUR BUCKET NAME/ada.jpg            --> downloads the image from the bucket to the shell
	f> gsutil cp gs://YOUR BUCKET NAME/ada.jpg gs://YOUR BUCKET NAME/image-folder/     --> copies image from the bucket to a folder in the same bucket 
	g> gsutil ls gs://YOUR BUCKET NAME or gsutil ls gs://YOUR BUCKET/image-folder      --> lists all the instances in the respective folders
	h> gsutil acl ch -u AllUsers:R gs://YOUR BUCKET NAME/ada.jpg                       --> gives reding access to all user for the particular resource
	i> gsutil acl ch -d AllUsers gs://YOUR BUCKET NAME/ada.jpg                         --> removes reding access from all the users for the particular resource
	j> gsutil rm gs://YOUR BUCKET NAME/ada.jpg                                         --> removes selected resources from the bucket 

10>    Cloud Monitoring: Qwik Start:
	a> curl -sSO https://dl.google.com/cloudagents/add-monitoring-agent-repo.sh
sudo bash add-monitoring-agent-repo.sh                                 --> Run the Monitoring agent install script command in the SSH terminal of your VM instance to install the Cloud Monitoring agent.
	b> sudo apt-get update
	c> sudo apt-get install stackdriver-agent
	d> curl -sSO https://dl.google.com/cloudagents/add-logging-agent-repo.sh
sudo bash add-logging-agent-repo.sh                                    --> Run the Logging agent install script command in the SSH terminal of your VM instance to install the Cloud Logging agent
	e> sudo apt-get update
	f> sudo apt-get install google-fluentd

11>   Cloud Functions: Qwik Start - (shell)
	a> mkdir gcf_hello_world        --> makes a directory in gcloud shell
	b> cd gcf_hello_world
	c> nano index.js  and pasting the following code in index.js <     /**
* Background Cloud Function to be triggered by Pub/Sub.
* This function is exported by index.js, and executed when
* the trigger topic receives a message.
*
* @param {object} data The event payload.
* @param {object} context The event metadata.
*/
exports.helloWorld = (data, context) => {
const pubSubMessage = data;
const name = pubSubMessage.data
    ? Buffer.from(pubSubMessage.data, 'base64').toString() : "Hello World";

console.log(`My Cloud Function: ${name}`);
};         >                                                                --> this creates a function
	d> gsutil mb -p [PROJECT_ID] gs://[BUCKET_NAME]                     --> creating a storage bucket
	e> gcloud functions deploy helloWorld \
  --stage-bucket [BUCKET_NAME] \
  --trigger-topic hello_world \
  --runtime nodejs8                                                          --> Deploy the function to a pub/sub topic named hello_world,
	f> gcloud functions describe helloWorld                              --> verifying the status of the function
	g> DATA=$(printf 'Hello World!'|base64) && gcloud functions call helloWorld --data '{"data":"'$DATA'"}'         --> Testing the function
	i> gcloud functions logs read helloWorld                             --> the logs to chcek the output

12> 	Google Cloud Pub/Sub: Qwik Start - Command Line:
	a> gcloud pubsub topics create myTopic                               --> creates a topic named myTopic
	b> gcloud pubsub topics list                        		     --> lists out all the topics created
	c> gcloud pubsub topics delete myTopic	        		     --> deletes the topic myTopic
	d> gcloud pubsub subscriptions create --topic myTopic mySubscription --> creates a subscription to the topic myTopic with the name mySubscription
	e> gcloud pubsub topics list-subscriptions myTopic                   --> lists out all the subscription to the topic myTopic
	f> gcloud pubsub subscriptions delete mySubscription                 --> delete a subscription named mySubscription
	g> gcloud pubsub topics publish myTopic --message "a"                --> publishes a message to the topic myTopic
	h> gcloud pubsub subscriptions pull mySubscription --auto-ack        --> pulls message one by one from the topic as no flag is added, here auto-ack is also a flag which gives and output in the form of boxes. once you get all the pulls, next pull request will give an error, showing 0 messages in log !
	i> gcloud pubsub subscriptions pull mySubscription --auto-ack --limit=3  --> this pulls all the 3 messages at once due to the flag used limit=3

13> Multiple vpc network:
-------- Create the privatenet network and its subnet:
	a> gcloud compute networks create privatenet --subnet-mode=custom     --> creating a VPC(virtual private cloud) named privatenet
	b> gcloud compute networks subnets create privatesubnet-us --network=privatenet --region=us-central1 --range=172.16.0.0/24    --> creaing a subnet in network privatenet, in region us-central1
	c> gcloud compute networks subnets create privatesubnet-eu --network=privatenet --region=europe-west4 --range=172.20.0.0/20   --> similarly creating a sub network inside vpc(privatenet), in region europe-west4
	d> gcloud compute networks list					      --> lists all the VPC's present in the project.
	e> gcloud compute networks subnets list --sort-by=NETWORK             --> lists all the subnets available in th project, sorted by network
-------- creating firewall rule for network or vpc: 
	f> gcloud compute firewall-rules create privatenet-allow-icmp-ssh-rdp --direction=INGRESS --priority=1000 --network=privatenet --action=ALLOW --rules=icmp,tcp:22,tcp:3389 --source-ranges=0.0.0.0/0
	g> gcloud compute firewall-rules list --sort-by=NETWORK		      --> lists all the firewall rules sorted by network
-------- making a vm in privatenet-us using the Cloud Shell command line.
	h> gcloud compute instances create privatenet-us-vm --zone=us-central1-c --machine-type=n1-standard-1 --subnet=privatesubnet-us      --> create the privatenet-us-vm instance:
	i> gcloud compute instances list --sort-by=ZONE                       --> lists all the vm instances present in the project sorted by zone 
-------- Explore the connectivity between VM instances:
	j> ping -c 3 <Enter mynet-eu-vm's external IP here> 		      --> open (ssh terminal ) for one vm in one vpc and ping to the external ip of other vm in some other vpc (it works!); 
	when you ping the internal ip address of a vm in same zone, it will work! but for other vm in other vpc it wont work
-------- Create a VM instance with multiple network interfaces:
	i> in (Management, security, disks, networking, sole tenancy.) > (Networking.) For Network interfaces, click the pencil icon to edit. and make it for all the vpc, now you can log into its ssh and use the internal ips of other vms to ping them as we have changed the networking of our vm!
	j> IN SSH OF A VM:
		1> sudo ifconfig                    --> o list the network interfaces within the VM instance:
		2> ping -c 3 privatenet-us-vm	    --> You are able to ping privatenet-us-vm (from a newly created vm, on any other zone which has multiple network interface) by its name because VPC networks have an internal DNS service that allows you to address instances by their DNS names rather than their internal IP addresses. When an internal DNS query is made with the instance hostname, it resolves to the primary interface (nic0) of the instance. Therefore, this only works for privatenet-us-vm in this case.
		2> ip route			    --> To list the routes for vm-appliance instance

14> introduction to docker:
	a> docker run hello-world                   --> The docker daemon searched for the hello-world image, didn't find the image locally, pulled the image from a public registry called Docker Hub, created a container from that image, and ran the container for you. 
	b> docker images			    -->  to take a look at the container image, which the docker daemon pulled from Docker Hub.
	c> docker ps				    --> to take a look at currently running containers.
	d> docker ps -a 			    --> to take a look at detailed info about the log of running containers.
-------- -- building a docker image ----------------------
	e> mkdir test && cd test		    --> making a folder where we can do our testing and learning 
	f> cat > Dockerfile <<EOF
# Use an official Node runtime as the parent image
FROM node:6

# Set the working directory in the container to /app
WORKDIR /app

# Copy the current directory contents into the container at /app
ADD . /app

# Make the container's port 80 available to the outside world
EXPOSE 80

# Run app.js using node when the container launches
CMD ["node", "app.js"]
EOF						     --> making a file named docker, which is read by docker daemon while building an image 
	g> cat > app.js <<EOF
const http = require('http');

const hostname = '0.0.0.0';
const port = 80;

const server = http.createServer((req, res) => {
    res.statusCode = 200;
      res.setHeader('Content-Type', 'text/plain');
        res.end('Hello World\n');
});

server.listen(port, hostname, () => {
    console.log('Server running at http://%s:%s/', hostname, port);
});

process.on('SIGINT', function() {
    console.log('Caught interrupt signal and will exit');
    process.exit();
});
EOF							--> this code is of a simple server which returns hello world under the file app.js
	g> docker build -t node-app:0.1 . 		--> this builds an image with name node-app and tag 0.1 to help in identification of images created
	h> docker images				--> lists all the images built.
-------- -- running the images ------------------------
	i> docker run -p 4000:80 --name my-app node-app:0.1	-->this code to run containers based on the image you built;  The --name flag allows you to name the container if you like. The -p instructs Docker to map the host's port 4000 to the container's port 80. Now you can reach the server at http://localhost:4000. Without port mapping, you would not be able to reach the container at localhost.	
	j> curl http://localhost:4000			--> to test the server
	h> docker stop my-app && docker rm my-app       --> stops the my-app running and delets the my-app container 
	i> docker run -p 4000:80 --name my-app -d node-app:0.1   -->  to start the container in the backgroun
	j> docker build -t node-app:0.2 .		--> this builds a new image with name node-app and tag 0.2 for identification purpose, this is done after altering the app.js file 
	k> docker run -p 8080:80 --name my-app-2 -d node-app:0.2   --> this runs a container,Notice how we map the host's port 8080 instead of 80. We can't use host port 4000 because it's already in use.
--------- -- debugging ---------------------------------
	l> docker logs -f [container_id]         	--> (2-3 letters can be used to reffer to the container id's)You can look at the logs of a container using docker logs [container_id]. If you want to follow the log's output as the container is running, use the -f option.
	m> docker exec -it [container_id] bash          --> Sometimes you will want to start an interactive Bash session inside the running container. You can use docker exec to do this. Open another terminal (in Cloud Shell, click the + icon) and enter the following command. (inside the container you will find both the files DockerFile and the app.js).
	n> exit  					--> to come out of the interactive Bash session use the exit command
	o> docker inspect [container_id]		--> examine a container's metadata in Docker by using Docker inspect
	p> docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [container_id]		--> Use --format to inspect specific fields from the returned JSON.
--------- -- publishing -------------------------------- ( pushing your image to the Google Container Registry (gcr))
	q> gcloud config list project
	r> docker tag node-app:0.2 gcr.io/[project-id]/node-app:0.2		--> Tag node-app:0.2. Replace [project-id] with your configuration.
	s> docker push gcr.io/[project-id]/node-app:0.2				--> pushing the image to gcr
	t> docker stop $(docker ps -q) 
docker rm $(docker ps -aq)				--> stop and remove all the containers.
	u> docker rmi node-app:0.2 gcr.io/[project-id]/node-app node-app:0.1
docker rmi node:6
docker rmi $(docker images -aq) # remove remaining images
docker images						-->  remove the child images (of node:6) before you remove the node image. Replace [project-id].
	v> docker pull gcr.io/[project-id]/node-app:0.2
docker run -p 4000:80 -d gcr.io/[project-id]/node-app:0.2
curl http://localhost:4000				-->  Pull the image and run it.
 

15> Orchestrating the Cloud with Kubernetes:
	a> gcloud config set compute/zone us-central1-b	--> set the compute zone
	b> gcloud container clusters create io		--> starts a cluster named io
-------- -- (Get the sample code)---------
	c> gsutil cp -r gs://spls/gsp021/* .		--> cloning the github repo
	d> cd orchestrate-with-kubernetes/kubernetes	--> changing the directory
	e> kubectl create deployment nginx --image=nginx:1.10.0			--> creates deployment
	f> kubectl get pods				--> views all the running containers inside their pods
	G> kubectl expose deployment nginx --port 80 --type LoadBalancer	--> exposing outside the container
	h> kubectl get services				--> gives details about the deployed instances 
	i> cat pods/monolith.yaml			--> shows the configuration file for the monolith pod creation 
	j> kubectl create -f pods/monolith.yaml		--> to create a pod named monolith
	k> kubectl get pods 				--> list all pods running in the default namespace:
	l> kubectl describe pods monolith		--> gives the details about the pod named monolith
	m> kubectl port-forward monolith 10080:80	--> from different terminal use this command; used for setting up port forwarding rule
	n> curl http://127.0.0.1:10080			--> from terminal 1 making requests to the pod . 
	o> curl http://127.0.0.1:10080/secure		--> hit a secure endpoint on the pod.
	p> curl -u user http://127.0.0.1:10080/login	--> Try logging in to get an auth token back from the monolith: At the login prompt, use the super-secret password "password" to login. Logging in caused a JWT token to print out. Since Cloud Shell does not handle copying long strings well, create an environment variable for the token.
	q> TOKEN=$(curl http://127.0.0.1:10080/login -u user|jq -r '.token')	--> inter the super secret password " password "
	r> curl -H "Authorization: Bearer $TOKEN" http://127.0.0.1:10080/secure	--> hitting secure endpoint of the pod
	s> kubectl logs -f monolith			--> on 3rd terminal, use the -f flag to get a stream of the logs happening in real-time 
	t> kubectl exec monolith --stdin --tty -c monolith /bin/sh		--> to run an interactive shell inside the Monolith Pod.
-------- -- (Creating a service + creating a secure monolith pod) ---------------
	u> cd ~/orchestrate-with-kubernetes/kubernetes
	v> cat pods/secure-monolith.yaml		-->  monolith service configuration file
	w> kubectl create secret generic tls-certs --from-file tls/
kubectl create configmap nginx-proxy-conf --from-file nginx/proxy.conf
kubectl create -f pods/secure-monolith.yaml		--> create a secure pod
	x> cat services/monolith.yaml			--> we have to expose the secure-monolith Pod externally> this file is monolith service configuration file
	y> kubectl create -f services/monolith.yaml	--> creates monolith service from monolith service configuration file.  to expose the nodeport here because this is how you'll forward external traffic from port 31000 to nginx (on port 443)
	z> gcloud compute firewall-rules create allow-monolith-nodeport \
  --allow=tcp:31000					-->  allow traffic to the monolith service on the exposed nodeport
	aa> gcloud compute instances list		--> gives details about all the instances
	ab> curl -k https://<EXTERNAL_IP>:31000		--> try hitting monolith service using curl (k flag is used to skip certificate validation)
	ac> kubectl get pods -l "app=monolith"		--> shows all the running pods with specified tag
	ad> kubectl get pods -l "app=monolith,secure=enabled"			--> shows pod with both the tags 
	ae> kubectl label pods secure-monolith 'secure=enabled'
kubectl get pods secure-monolith --show-labels		--> label the monolith pod with label "security=enabled"
	af> kubectl describe services monolith | grep Endpoints			--> shows the list of endpoints of the monolith service from correctly labeled pods
	ag> gcloud compute instances list
curl -k https://<EXTERNAL_IP>:31000			
-------- -- deploying applications using kubernetes -----------------------------
	ah> cat deployments/auth.yaml			--> auth deployment configuration file
	ai> kubectl create -f deployments/auth.yaml	--> creates a deployment object
	aj> kubectl create -f services/auth.yaml	--> creates auth service
	ak> kubectl create -f deployments/hello.yaml
kubectl create -f services/hello.yaml			--> create and expose hello deployment
	al> kubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf
kubectl create -f deployments/frontend.yaml
kubectl create -f services/frontend.yaml		--> creating and exposing frontend deployment
	am> kubectl get services frontend		--> grab ip for frontedn
	an> curl -k https://<EXTERNAL-IP>		

16> Managing Deployments Using Kubernetes Engine: 
	a> gcloud config set compute/zone us-central1-a	--> set zone
	b> gsutil -m cp -r gs://spls/gsp053/orchestrate-with-kubernetes .
cd orchestrate-with-kubernetes/kubernetes		--> getting sample code 
	c> gcloud container clusters create bootcamp --num-nodes 5 --scopes "https://www.googleapis.com/auth/projecthosting,storage-rw"		--> Create a cluster with five n1-standard-1 nodes
	d> kubectl explain deployment			--> explaines about deployment
	e> kubectl explain deployment --recursive	--> shows all the fields available in deployment 
	f> kubectl explain deployment.metadata.name	--> explains the field deployment.metadata.name clearly
-------- -- Creating a deployment -------------------------------------------------
	g> vi deployments/auth.yaml			--> opens the file deployments/auth.yaml in vi editor
	h> i 						--> starts editting option for the opened file
	i> press esc and type ":wq" to exit the editor
	j> kubectl create -f deployments/auth.yaml	-->  it will make one pod that conforms to the data in the Deployment manifest.
	k> kubectl get deployments			--> see the deployments done
	l> kubectl get replicasets			--> depicts all the replica sets (replica set is defined in auth.yaml file)
 	m> kubectl get pods				--> to view all the pods created
	n> kubectl create -f services/auth.yaml		--> create a service for auth.yaml
	o> kubectl create -f deployments/hello.yaml
kubectl create -f services/hello.yaml			--> creating and exposing the hello deployment
	p> kubectl create secret generic tls-certs --from-file tls/
kubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf
kubectl create -f deployments/frontend.yaml
kubectl create -f services/frontend.yaml		--> to create and expose the frontend Deploymen
	q> kubectl get services frontend		--> get details about frontend deployment 
	r> curl -ks https://<EXTERNAL-IP>		--> test the frontend deployment
	s> curl -ks https://`kubectl get svc frontend -o=jsonpath="{.status.loadBalancer.ingress[0].ip}"`					--> use the output templating feature of kubectl to use curl as a one-liner:
-------- -- Scaling a deployment -------------------------------------------------
	t> kubectl explain deployment.spec.replicas  	--> explains the field deployment.spec.replicas
	u> kubectl scale deployment hello --replicas=5	--> replicating the deployment to 5 
	v> kubectl get pods | grep hello- | wc -l	--> Verify that there are now 5 hello Pods running
	w> kubectl scale deployment hello --replicas=3	--> scaling deployment to 3
	x> kubectl get pods | grep hello- | wc -l	--> check the number of hello pods
-------- -- Rolling update -------------------------------------------------------
	y> kubectl edit deployment hello		--> opens up the deployments configuration yaml file to update the image in the container section of the deployment
	 <Change the image in the containers section of the Deployment which show the version of the deployment> <Once you save out of the editor, the updated Deployment will be saved to your cluster and Kubernetes will begin a rolling update.> 
	z> kubectl get replicaset			--> shows the replicaset
	aa> kubectl rollout history deployment/hello	--> see a new entry in the rollout history. shows 1 enter 2 here one and 2 are the deployments
	ab> kubectl rollout pause deployment/hello	--> to pause a rolling update
	ac> kubectl rollout status deployment/hello	--> verifying the current state of rollout
	ad> kubectl get pods -o jsonpath --template='{range .items[*]}{.metadata.name}{"\t"}{"\t"}{.spec.containers[0].image}{"\n"}{end}'	--> to verify the pods version
	ae> kubectl rollout resume deployment/hello	--> to resume the paused rolling
	af> kubectl rollout undo deployment/hello	--> to rollback the update
	ag> kubectl get pods -o jsonpath --template='{range .items[*]}{.metadata.name}{"\t"}{"\t"}{.spec.containers[0].image}{"\n"}{end}'	--> to verify that all the pods have rolled back to previous version
-------- -- canary deployment -----------------------------------------------------
	ah> cat deployments/hello-canary.yaml		--> creating a new canary deployment for the new version
	ai> kubectl create -f deployments/hello-canary.yaml											--> creating a canary deployment 
	aj> kubectl get deployments			--> After the canary deployment is created, you should have two deployments, hello and hello-canary. Verify it with this kubectl command 
	ak> curl -ks https://`kubectl get svc frontend -o=jsonpath="{.status.loadBalancer.ingress[0].ip}"`/version				--> verifying the canary deployment (Run this several times and you should see that some of the requests are served by hello 1.0.0 and a small subset (1/4 = 25%) are served by 2.0.0.)
-------- -- Blue green deployment -------------------------------------------------
	al> kubectl apply -f services/hello-blue.yaml	--> Use the existing hello service, but update it so that it has a selector app:hello, version: 1.0.0. The selector will match the existing "blue" deployment. But it will not match the "green" deployment because it will use a different version.
	am> kubectl create -f deployments/hello-green.yaml											--> creating the green deployment 
	an> curl -ks https://`kubectl get svc frontend -o=jsonpath="{.status.loadBalancer.ingress[0].ip}"`/version				--> check still the last deployment is in use i.e. v0.0.0.1
	ao> kubectl apply -f services/hello-green.yaml	--> update the service to point to the new version
	ap> curl -ks https://`kubectl get svc frontend -o=jsonpath="{.status.loadBalancer.ingress[0].ip}"`/version				--> check the version in use it will return version 2
--------- -- Blue green rollback ---------------------------------------------------
	aq> kubectl apply -f services/hello-blue.yaml	--> applying the blue version again
	ar> curl -ks https://`kubectl get svc frontend -o=jsonpath="{.status.loadBalancer.ingress[0].ip}"`/version				--> check the version
	
17> Continuous Delivery with Jenkins in Kubernetes Engine (https://www.qwiklabs.com/focuses/1104?parent=catalog)

18> Introduction to SQL for BigQuery and Cloud SQL: 

19> Cloud Source Repositories: Qwik Start:
	a> gcloud source repos create REPO_DEMO		--> creates a new Cloud Source Repository named REPO_DEMO
	b> gcloud source repos clone REPO_DEMO		--> cloning the new cloud source repository to a local repo in cloud shell (The gcloud source repos clone command adds Cloud Source Repositories as a remote named origin and clones it into a local Git repository.)
-------- -- push to google cloud source repository ----------------------------------
	c> cd REPO_DEMO					--> move inside the cloned repo
	d> echo 'Hello World!' > myfile.txt		--> make a file named myfile.txt and save Hello world! into it
	e> git config --global user.email "you@example.com"											--> configuring the global email address
	f> git config --global user.name "name"		--> configures global user name for the repo
	g> git commit -m "First file using Cloud Source Repositories" myfile.txt								--> code to commit the changes 
	h>git push origin master			--> pushing changes to the origin master 
	
20> Deploy Kubernetes Load Balancer Service with Terraform : 
	a> wget https://releases.hashicorp.com/terraform/0.13.0/terraform_0.13.0_linux_amd64.zip						--> updating terraform by downloading the newest version
	b> unzip terraform_0.13.0_linux_amd64.zip 	--> unzipping the downloaded folder
	c> sudo mv terraform /usr/local/bin/		--> moving the executable to bin folder 
	d> terraform -v					--> check the version of the terraform
	e> gsutil -m cp -r gs://spls/gsp233/* .		--> cloning the sample code
	f> cd tf-gke-k8s-service-lb			--> navigating to required folder 
	g> cat main.tf					--> Variables are defined for region, zone, and network_name. These will be used to create the Kubernetes cluster. The Google Cloud provider will let us create resources in this project
	h> cat k8s.tf					--> The script configures a Kubernetes provider with Terraform and creates the service, namespace and a replication_controller resource. The script returns an nginx service IP as a output.
	i> terraform init				--> The terraform init command is used to initialize a working directory containing the Terraform configuration files.	
	j> terraform apply				-->  used to apply the changes required to reach the desired state of the configuration. (The configuration is listed into k8s.tf file)
	
21> IAM Custom Roles:
	a> echo $DEVSHELL_PROJECT_ID			--> this shows the project id
	b> gcloud iam list-testable-permissions //cloudresourcemanager.googleapis.com/projects/$DEVSHELL_PROJECT_ID				--> lists all the uncountable permissions associated with your project
	c> gcloud iam roles describe [ROLE_NAME]	--> gets the role's metadata (which contains the role ID and permissions contained in the roles).
	d> gcloud iam list-grantable-roles //cloudresourcemanager.googleapis.com/projects/$DEVSHELL_PROJECT_ID					-->  list grantable roles from your project
-------- -- creating a cutom role (using yaml file) ----------------------------------------
	e> nano role-definition.yaml
	f> title: "Role Editor"
description: "Edit access for App Versions"
stage: "ALPHA"
includedPermissions:
- appengine.versions.create
- appengine.versions.delete				--> in above two command we created a yaml file which contains the description about the roles
	g> gcloud iam roles create editor --project $DEVSHELL_PROJECT_ID \
--file role-definition.yaml				--> this command creates the editor role as specified in the role-definition.yaml file.
-------- -- creating a custom role (using flags) ------------------------------------------
	i> gcloud iam roles create viewer --project $DEVSHELL_PROJECT_ID \
--title "Role Viewer" --description "Custom role description." \
--permissions compute.instances.get,compute.instances.list --stage ALPHA									--> this command creates a viewer role as per given specifications
	j> gcloud iam roles list --project $DEVSHELL_PROJECT_ID											--> lists all the custom roles
-------- -- updating the custome role using yamal file -----------------------------------
	k> gcloud iam roles describe [ROLE_ID] --project $DEVSHELL_PROJECT_ID									--> Get the current definition for the role by executing the following gcloud command, replacing [ROLE_ID] with editor.
	l> nano new-role-definition.yaml		--> creates a new file named as new-role-definition.yaml
	m> - storage.buckets.get
- storage.buckets.list					--> copy the output from the line <k> and add these 2 lines in the permission section of the openend file
	n> gcloud iam roles update [ROLE_ID] --project $DEVSHELL_PROJECT_ID --file new-role-definition.yaml					--> updates the editor role
--------- -- updating the custom role using the flags -------------------------------------
	o> gcloud iam roles update viewer --project $DEVSHELL_PROJECT_ID --add-permissions storage.buckets.get,storage.buckets.list		--> adds the given two permissions to the previous viewer role. more permissions can be set by adding more permissions after commas.
	p> gcloud iam roles update viewer --project $DEVSHELL_PROJECT_ID --stage DISABLED							--> When a role is disabled, any policy bindings related to the role are inactivated, meaning that the permissions in the role will not be granted, even if you grant the role to a user.
	q> gcloud iam roles delete viewer --project $DEVSHELL_PROJECT_ID									--> Use the gcloud iam roles delete command to delete a custom role. Once deleted the role is inactive and cannot be used to create new IAM policy bindings.
	r> gcloud iam roles undelete viewer --project $DEVSHELL_PROJECT_ID									--> Within the 7 days window you can undelete a role. Deleted roles are in a DISABLED state. You can make it available again by updating the --stage flag:

22> Service accounts and roles: Fundamentals
	a> gcloud iam service-accounts create my-sa-123 --display-name "my service account"							--> create a service account named my-sa-123
	b> gcloud projects add-iam-policy-binding $DEVSHELL_PROJECT_ID \
    --member serviceAccount:my-sa-123@$DEVSHELL_PROJECT_ID.iam.gserviceaccount.com --role roles/editor						--> granting editor access to the created service account
	c> after creating a vm and linking it to the service account
	d> sudo apt-get update				--> download and updates the packages lists
	e> sudo apt-get install virtualenv		--> Python virtual environments are used to isolate package installation from the system.
	virtualenv -p python3 venv	
	f> sudo apt-get install -y git python3-pip
pip install google-cloud-bigquery
pip install pandas
Now create the example Python 				--> installing the other dependencies
	g> echo "
from google.auth import compute_engine
from google.cloud import bigquery

credentials = compute_engine.Credentials(
    service_account_email='YOUR_SERVICE_ACCOUNT')

query = '''
SELECT
  year,
  COUNT(1) as num_babies
FROM
  publicdata.samples.natality
WHERE
  year > 2000
GROUP BY
  year
'''

client = bigquery.Client(
    project='YOUR_PROJECT_ID',
    credentials=credentials)
print(client.query(query).to_dataframe())
" > query.py						--> creates a file named query.py 
	h> sed -i -e "s/YOUR_PROJECT_ID/$(gcloud config get-value project)/g" query.py								--> Add the Project ID to query.py with
	i> sed -i -e "s/YOUR_SERVICE_ACCOUNT/bigquery-qwiklab@$(gcloud config get-value project).iam.gserviceaccount.com/g" query.py		--> Add the service account email to query.py
	j> python query.py				--> runs the python file

23> VPC Network peering:
	a> gcloud config set project <PROJECT_ID2>				--> set project id 
	b> gcloud compute networks create network-a --subnet-mode custom	--> create a vpc
	c> gcloud compute networks subnets create network-a-central --network network-a \
    --range 10.0.0.0/16 --region us-central1					--> creating a subnet
	d> gcloud compute instances create vm-a --zone us-central1-a --network network-a --subnet network-a-central				--> creating a vpc in the given tag network
	e> gcloud compute firewall-rules create network-a-fw --network network-a --allow tcp:22,icmp 						--> Run the following to enable SSH and icmp, because you'll need a secure shell to communicate with VMs during connectivity testing:
	f> as same as above we made one more vpc in other project with same config, a subnet, a vm and firewall, then we peered both the networks internally and then we pinged the vm in first network via its internal ip and also with the external ip. and i verified that when pinged with internal ip, the ping time was less

24> Getting Started with Cloud KMS:
	a> BUCKET_NAME=<YOUR_NAME>_enron_corpus					--> setting up an environment variable
	b> gsutil mb gs://${BUCKET_NAME}					--> creating a bucket
	c> gsutil cp gs://enron_emails/allen-p/inbox/1. .			--> downloads the source file locally
	d> tail 1.								--> to verify the email text is there
	e> gcloud services enable cloudkms.googleapis.com			--> enabling the Kms service
	f> KEYRING_NAME=test CRYPTOKEY_NAME=qwiklab				--> setting up environment variables
	g> gcloud kms keyrings create $KEYRING_NAME --location global		--> creating a keyring for now we are using using a global location, but it could also be set to a specific region.
	i> gcloud kms keys create $CRYPTOKEY_NAME --location global \
      --keyring $KEYRING_NAME \
      --purpose encryption							--> create crypto key
	j> PLAINTEXT=$(cat 1. | base64 -w0)					--> setting environment variables (Base-64 encoding allows binary data to be sent to the API as plaintext. This command works for images, videos, or any other kind of binary data.)
	k> curl -v "https://cloudkms.googleapis.com/v1/projects/$DEVSHELL_PROJECT_ID/locations/global/keyRings/$KEYRING_NAME/cryptoKeys/$CRYPTOKEY_NAME:encrypt" \
  -d "{\"plaintext\":\"$PLAINTEXT\"}" \
  -H "Authorization:Bearer $(gcloud auth application-default print-access-token)"\
  -H "Content-Type: application/json"						--> encryption code (The encrypt action will return a different result each time even when using same text and key.) The response will be a JSON payload containing the encrypted text in the attribute ciphertext.
	l> curl -v "https://cloudkms.googleapis.com/v1/projects/$DEVSHELL_PROJECT_ID/locations/global/keyRings/$KEYRING_NAME/cryptoKeys/$CRYPTOKEY_NAME:encrypt" \
  -d "{\"plaintext\":\"$PLAINTEXT\"}" \
  -H "Authorization:Bearer $(gcloud auth application-default print-access-token)"\
  -H "Content-Type:application/json" \
| jq .ciphertext -r > 1.encrypted						--> the command-line utility jq. The response from the previous call can be piped into jq, which can parse out the ciphertext property to the file 1.encrypted.
	m> curl -v "https://cloudkms.googleapis.com/v1/projects/$DEVSHELL_PROJECT_ID/locations/global/keyRings/$KEYRING_NAME/cryptoKeys/$CRYPTOKEY_NAME:decrypt" \
  -d "{\"ciphertext\":\"$(cat 1.encrypted)\"}" \
  -H "Authorization:Bearer $(gcloud auth application-default print-access-token)"\
  -H "Content-Type:application/json" \
| jq .plaintext -r | base64 -d							--> code for decrypting
	n> gsutil cp 1.encrypted gs://${BUCKET_NAME}				--> upload the encrypted file to bucket
-------- -- Back up data on the Command Line ---------------------------------------
	o> gsutil -m cp -r gs://enron_emails/allen-p .				--> download the folder
	p> MYDIR=allen-p
FILES=$(find $MYDIR -type f -not -name "*.encrypted")
for file in $FILES; do
  PLAINTEXT=$(cat $file | base64 -w0)
  curl -v "https://cloudkms.googleapis.com/v1/projects/$DEVSHELL_PROJECT_ID/locations/global/keyRings/$KEYRING_NAME/cryptoKeys/$CRYPTOKEY_NAME:encrypt" \
    -d "{\"plaintext\":\"$PLAINTEXT\"}" \
    -H "Authorization:Bearer $(gcloud auth application-default print-access-token)" \
    -H "Content-Type:application/json" \
  | jq .ciphertext -r > $file.encrypted
done
gsutil -m cp allen-p/inbox/*.encrypted gs://${BUCKET_NAME}/allen-p/inbox	--> This script loops over all the files in a given directory, encrypts them using the KMS API, and uploads them to Cloud Storage.

24> Setting up a Private Kubernetes Cluster: 
	a> gcloud config set compute/zone us-central1-a				--> setting zone of the cluster
	b> gcloud beta container clusters create private-cluster \
    --enable-private-nodes \
    --master-ipv4-cidr 172.16.0.16/28 \
    --enable-ip-alias \
    --create-subnetwork ""							--> creating a private cluster (When you enable IP aliases, you let Kubernetes Engine automatically create a subnetwork for you.)
	c> gcloud compute networks subnets list --network default		--> lists out all the default subnets in the cluster
	d> gcloud compute networks subnets describe [SUBNET_NAME] --region us-central1								--> Gives info about any particular subnet
-------- -- Enabling a master authorized network -------------------------------------------
	e> gcloud compute instances create source-instance --zone us-central1-a --scopes 'https://www.googleapis.com/auth/cloud-platform'	--> creating a vm to connect to the cluster
	f> gcloud compute instances describe source-instance --zone us-central1-a | grep natIP							--> prints the external ip address of the source instance
	g> gcloud container clusters update private-cluster \
    --enable-master-authorized-networks \
    --master-authorized-networks [MY_EXTERNAL_RANGE]				--> add the external ip address of the above created vm to the cluster for communication. (format is natIP/32)
	h> gcloud compute ssh source-instance --zone us-central1-a		--> SSH into source-instance
	i> gcloud components install kubectl (this doesn't work because (You cannot perform this action because the Cloud SDK component manager is disabled for this installation)) try to install kubectl component with: sudo apt-get install kubectl.
	j> kubectl get nodes --output yaml | grep -A4 addresses  <or> kubectl get nodes --output wide						--> erify that your cluster nodes do not have external IP addresses
	k> exit
	l> gcloud container clusters delete private-cluster --zone us-central1-a--> to delete the cluster 
-------- -- Creating a private cluster that uses a custom subnetwork ------------------------
	m> gcloud compute networks subnets create my-subnet \
    --network default \
    --range 10.0.4.0/22 \
    --enable-private-ip-google-access \
    --region us-central1 \
    --secondary-range my-svc-range=10.0.32.0/20,my-pod-range=10.4.0.0/14 	--> creates a subnet for the future private cluster
	n> gcloud beta container clusters create private-cluster2 \
    --enable-private-nodes \
    --enable-ip-alias \
    --master-ipv4-cidr 172.16.0.32/28 \
    --subnetwork my-subnet \
    --services-secondary-range-name my-svc-range \
    --cluster-secondary-range-name my-pod-range					--> creates a private cluster for the above subnet
	n> gcloud container clusters update private-cluster2 \
    --enable-master-authorized-networks \
    --master-authorized-networks [MY_EXTERNAL_RANGE]				--> update your private cluster with the external ip address of the vm created above where the (format is natIP/32).
	o> gcloud compute ssh source-instance --zone us-central1-a		--> ssh into the source instance
	p> gcloud container clusters get-credentials private-cluster2 --zone us-central1-a							--> getting access to the private cluster 2 from ssh of the vm
	q> kubectl get nodes --output yaml | grep -A4 addresses			--> again verifying, that the private-cluser-2 doesn't have any external ip address.

25> Setting up Jenkins on Kubernetes Engine:
	a> gcloud config set compute/zone us-east1-d
	b> git clone https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.git
	c> cd continuous-deployment-on-kubernetes
-------- -- creating a kubernetes cluster ------------------------------------
	d> gcloud container clusters create jenkins-cd \
--num-nodes 2 \
--machine-type n1-standard-2 \
--scopes "https://www.googleapis.com/auth/projecthosting,cloud-platform" 	--> creates a kubernetes cluster
	e> gcloud container clusters list					--> lists all the clusters
	f> gcloud container clusters get-credentials jenkins-cd			--> Get the credentials for your cluster. Kubernetes Engine uses these credentials to access your newly provisioned cluster.
	g> kubectl cluster-info							--> get info about the cluster
	i> helm repo add stable https://charts.helm.sh/stable			--> Add Helm's stable chart repository:
	j> helm repo update							--> Update the repo to ensure you get the latest list of charts
-------- -- Configure and install jenkins -----------------------------------
	k> helm install cd stable/jenkins -f jenkins/values.yaml --version 1.2.2 --wait								-->Use the Helm CLI to deploy the chart with your configuration set
	l> kubectl get pods							-->  ensure the Jenkins pod goes to the Running state and the container is in the READY state
	m> export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/component=jenkins-master" -l "app.kubernetes.io/instance=cd" -o jsonpath="{.items[0].metadata.name}")
kubectl port-forward $POD_NAME 8080:8080 >> /dev/null &				--> setup port forwarding to the Jenkins UI from the Cloud Shell
	n> kubectl get svc							--> now check that jenkins service was created properly
	o> printf $(kubectl get secret cd-jenkins -o jsonpath="{.data.jenkins-admin-password}" | base64 --decode);echo				--> gets password for the jenkins web login

26> Continuous Delivery Pipelines with Spinnaker and Kubernetes Engine:

27> Deploying a Fault-Tolerant Microsoft Active Directory Environment

28> Deploying Memcached on Kubernetes Engine:

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


29> Weather Data in BigQuery

30> Classify Images of Clouds in the Cloud with AutoML Vision
	a> enable the Cloud AutoML API.
	b> export PROJECT_ID=$DEVSHELL_PROJECT_ID   				--> setting the environment variable for the project id
	c> export QWIKLABS_USERNAME=<USERNAME>  				--> setting the username environment variable
	d> gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="user:$QWIKLABS_USERNAME" \
    --role="roles/automl.admin"							--> assigning the permission to the api via granting the automl admin role to the user 
	e> gsutil mb -p $PROJECT_ID \
    -c standard    \
    -l us-central1 \
    gs://$PROJECT_ID-vcm/							--> creating the bucket for data storage
	f> export BUCKET=$PROJECT_ID-vcm					--> setting bucket name environment variable
	g> gsutil -m cp -r gs://spls/gsp223/images/* gs://${BUCKET}		--> copying the image data from the internet to the bucket
-------- -- Creating a dataset ----------------------------------------------
	h> gsutil cp gs://spls/gsp223/data.csv .				--> copying an old csv file to the bucket from internet
	i> sed -i -e "s/placeholder/${BUCKET}/g" ./data.csv			--> updating the csv file with the required data that we copied from the internet 
	J> gsutil cp ./data.csv gs://${BUCKET}					--> sending or copying the updated csv file to the bucket
	i> https://console.cloud.google.com/vision/datasets			--> go to autovision interface and create a new dataset
	j> select single lable classification and select the csv on the cloud to be imported after checking the option "Select a CSV file on Cloud Storage".
-------- -- Training the model ---------------------------------------------
	k> To train your clouds model, go to the Train tab and click Start Training. Enter a name for your model, or use the default auto-generated name. Leave Cloud-hosted selected, then click Continue. Set the node hours to 8.
-------- -- Deploy your model ----------------------------------------------
	l> Navigate to the Test & Use tab in the AutoML UI: Deploy model then Deploy.
-------- -- Generating predictions -----------------------------------------
	
31> Google Assistant: Build a Restaurant Locator with the Places API
	a> Basically here we have to make an action on our cloud platform, which takes voice input that what is your location, and some other questions and based on these questions it suggests you the best matches (i.e nearby restaurents on a map)
	b> The task is divided into 4 subtasks 1> creating a dialog flow connector/webhook (which will send request 200 to the function on cloud). 2> creating action/intent on the dialogflog (it will act as an input/output for the entire project) 
		3> APIs (a} Places API, this api provides pictures and informations of places; b} Geocoding API, this api converts the location obtained from the Places API to coordinates c} Maps JavaScript Api, This api uses the coordinates to plot the places on the map for the user as output)
		4> The last part consists of creating a function on cloud, which will show the plottings, Api key is created before it and is provided inside the function's codes.

32>  Introduction to APIs in Google: 
	a> https://console.cloud.google.com/apis/library/storage-api.googleapis.com?_ga=2.263178572.-1708473792.1539023585          --> ensure cloud api is enabled
	b> nano values.json
	c> {  "name": "<YOUR_BUCKET_NAME>",
   "location": "us",
   "storageClass": "multi_regional"
}										--> add this data into the values.json file it contains the data for the cloud bucket
	d> Authenticate and authorize the Cloud Storage JSON/REST API(authenticate from any of the sources < API keys, OAuth,service accounts>). select the api and gain the access token
	e> export OAUTH2_TOKEN=<YOUR_TOKEN>
	f> export PROJECT_ID=<YOUR_PROJECT_ID>
	g> curl -X POST --data-binary @values.json \
    -H "Authorization: Bearer $OAUTH2_TOKEN" \
    -H "Content-Type: application/json" \
    "https://www.googleapis.com/storage/v1/b?project=$PROJECT_ID" 		--> create a bucket using the rest api
	h> realpath demo-image.png						--> getting path to the image stored somewhere on the shell
	i> export OBJECT=<DEMO_IMAGE_PATH>					--> exporting the path 
	j> export BUCKET_NAME=<YOUR_BUCKET>					--> exporting the bucket name
	i> curl -X POST --data-binary @$OBJECT \
    -H "Authorization: Bearer $OAUTH2_TOKEN" \
    -H "Content-Type: image/png" \
    "https://www.googleapis.com/upload/storage/v1/b/$BUCKET_NAME/o?uploadType=media&name=demo-image"					--> uploading the image from the specefied location to the cloud storage, using cloud storage rest api
	
33> Google Assistant: Build an Application with Dialogflow and Cloud Functions

34> Google Assistant: Build a Youtube Entertainment App:
	
35> Extract, Analyze, and Translate Text from Images with the Cloud ML APIs
	a> creating an API key
	b> making a bucket and storing a picture in it
	c> {
  "requests": [
      {
        "image": {
          "source": {
              "gcsImageUri": "gs://my-bucket-name/sign.jpg"
          }
        },
        "features": [
          {
            "type": "TEXT_DETECTION",
            "maxResults": 10
          }
        ]
      }
  ]
}										--> this is the ocr-request.json file
	d> curl -s -X POST -H "Content-Type: application/json" --data-binary @ocr-request.json  https://vision.googleapis.com/v1/images:annotate?key=${API_KEY}--> calls the vision api text detection method. It returns (description & boundingPoly) of the text on the image.
	e> The OCR method is able to extract lots of text from our image, <output is obtained on the console>
	f> curl -s -X POST -H "Content-Type: application/json" --data-binary @ocr-request.json  https://vision.googleapis.com/v1/images:annotate?key=${API_KEY} -o ocr-response.json--> copies the response from the ocr to ocr-response.json file
	g> {
  "q": "your_text_here",
  "target": "en"
}										--> format of translation-request.json file
	h> STR=$(jq .responses[0].textAnnotations[0].description ocr-response.json) && STR="${STR//\"}" && sed -i "s|your_text_here|$STR|g" translation-request.json--> copies the data from the ocr-response.json to translation-request.json
	i> curl -s -X POST -H "Content-Type: application/json" --data-binary @translation-request.json https://translation.googleapis.com/language/translate/v2?key=${API_KEY} -o translation-response.json--> send request to translation api and stores response to translation-response.json
	j> {
  "document":{
    "type":"PLAIN_TEXT",
    "content":"your_text_here"
  },
  "encodingType":"UTF8"
}										--> format of n1-request.json file for sending request to nlp api
	k> STR=$(jq .data.translations[0].translatedText  translation-response.json) && STR="${STR//\"}" && sed -i "s|your_text_here|$STR|g" nl-request.json	--> copying the data from translation-response.json to ni-request.json in respective format
	l> curl "https://language.googleapis.com/v1/documents:analyzeEntities?key=${API_KEY}" \
  -s -X POST -H "Content-Type: application/json" --data-binary @nl-request.json	--> request sent to nlp api via n1-request.json


36> Classify Text into Categories with the Natural Language API:
	https://github.com/GoogleCloudPlatform/cloud-vision/tree/master/python/awwvision

37> App Dev: Setting up a Development Environment - Python:
	a> making a vm and starting ssh: >> sudo apt-get update : to update the Debian package list
	b> sudo apt-get install git						--> install git on vm
	c> sudo apt-get install python3-setuptools python3-dev build-essential	--> installing python on vm
	d> curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py +  sudo python3 get-pip.py							--> installing pip on the vm
	e> python3 --version   + pip3 --version    				--> checking versions
	f> git clone https://github.com/GoogleCloudPlatform/training-data-analyst								--> cloning the required repo on our vm
	g> cd ~/training-data-analyst/courses/developingapps/python/devenv/
	i> sudo python3 server.py						--> running the server code written in python now we can use the external ip provided by the virtual instance to see our server up and running
	j> sudo pip3 install -r requirements.txt				--> Install the Python packages needed to enumerate Compute Engine VM instance
	k> python3 list-gce-instances.py <PROJECT_ID> --zone=<YOUR_VM_ZONE>	--> running the list-gce-instances.py to enumerate the compute engine vm instances/

38> App Dev: Storing Application Data in Cloud Datastore - Python

39> App Dev: Storing Image and Video Files in Cloud Storage - Python
	a> git clone https://github.com/GoogleCloudPlatform/training-data-analyst--> cloning the code repo
	b> ln -s ~/training-data-analyst/courses/developingapps/v1.2/python/firebase ~/firebase							--> creating a soft link
	c> cd ~/firebase/start
	d> . prepare_environment.sh						--> prepare_environment.sh contains all the things to to installed on the shell. It runs and (Creates an App Engine application. Creates a Cloud Storage bucket named gs:[Project-ID]-media. Exports two environment variables: GCLOUD_PROJECT and GCLOUD_BUCKET. Runs > pip install -r requirements.txt. Creates entities in Cloud Datastore. Prints out the Project ID.)
	e> python run_server.py
	f> working with firebase (open firebase console > add your project via adding project id > accept the conditions .. > Build > Authentication > add sign in method > click Authorized domain > )
	g> Apply Firebase configuration to a web application			--> Click the web icon. Register your app - give your app a nick name, then click Register app.
	h> copy firebase SDK scripts 
	I> add it to index.html in  training_data_analyst/courses/developingapps/v1.2/python/firebase/start/quiz/webapp/static/client/.
	j> <script src="https://www.gstatic.com/firebasejs/7.5.2/firebase-auth.js"></script>    						--> add this below the previous lines
	k> test

40> App Dev - Deploying the Application into App Engine Flexible Environment - Java (same concepts <just like docker>)
	checkout (https://www.qwiklabs.com/focuses/1060?parent=catalog)

41> Cloud Profiler: Qwik Start (In this lab you will learn how to set up and use Stackdriver Profiler. First you'll download a sample Go program and run it with profiling enabled. Then you'll use the Cloud Profiler interface to explore the captured data.)
	a> Enable Cloud Profiler API
	b> Get a program to profile (This program is designed to to load the CPU as it runs, and configured to use Cloud Profiler. Cloud Profiler collects profiling data from the program as it runs and periodically saves it. Progress is indicated with a pair of messages:)
	c> Profile the code <profiler_quickstart>
	d> Start the Profiler interface by searching stackdriver profiler 


42> Fundamentals of Cloud Logging: 
	a> Setup and requirements installed (This web application generates logs for us to examine )
	b> Deployed the application
	c> Setting up the database and then again deploying the application
	d> Viewing and searching logs (checking the logs and exploring the types of metrices)
	e> using Filters to filter out the logs
	f> Log based metrics (types of metrices)
	g> Create a counter metric (setting up the filter> action > create Metric > after some steps the metric is created)
	h> Create a distribution metric
	i> View log metrics in the Cloud Monitoring Console
	j> Audit logging (Google Cloud provides Auditing of all Google Cloud resources by default. The audit logs answer the question "Who did what, when?" Let's look at Audit Logging, starting by creating a new Compute Engine (Compute Engine) virtual machine (VM). Launching a VM is an example of an audited privileged activity, so it generates logs.)
		we can check this in the activity tab and as well as in the logging tab.
	k> Exporting logs (creating sink to a big querry database and then using the big querry databse to apply querry on it to retrieve important results).

43> Using BigQuery and Cloud Logging to Analyze BigQuery Usage:
	a> Open BigQuery and create a dataset and run a querry on it "SELECT current_date"
	b> Setup Log Export from Cloud Logging
	c> Run example queries on google console so that we can generate enough logs to be displayed in the Big querry dataset
	d> Viewing the logs in BigQuery

44> Cloud Logging on Kubernetes Engine:
	a>  In this lab you will deploy a sample application to Kubernetes Engine that forwards log events to Cloud Logging using Terraform, a declarative Infrastructure as Code tool that enables configuration files to automate the deployment and evolution of infrastructure in the cloud. The configuration will also create a Cloud Storage bucket and a BigQuery dataset for exporting log data to.
	b> Architecture: (The Terraform configurations are going to build a Kubernetes Engine cluster that will generate logs and metrics that can be ingested by Stackdriver. The scripts will also build out Logging Export Sinks for Cloud Storage, BigQuery, and Cloud Pub/Sub)
	c> Clone demo (clone the repo containing the application, terraform files and other dependencies) <the 3 terraform files are main.tf, provider.tf, variable.tf>
	d> Deployment <make create> (this run the tf files)
	e> Validation: <make validate> for checking what the terraform had performed
	f> Logs in Cloud Logging: terraform did created some clusters which caused logs into the cloud logging dashboard
	g> Viewing Log Exports
	h> Logs in Cloud Storage: Log events can be stored in Cloud Storage, an object storage system suitable for archiving data
	i> Logs in BigQuery
	j> Teardown <make teardown> (Qwiklabs will take care of shutting down all the resources used for this lab, but hereâ€™s what you would need to do to clean up your own environment to save on cost and to be a good cloud citizen:)

45> Monitoring and Logging for Cloud Functions:
	a> Viewing Cloud Function logs & metrics in Cloud monitoring (create a cloud function )
	b> Create logs-based metric (open logging (select <CloudFunction>helloWorld  and in logname check cloud-functions> and and run the querry)from action button create a distribution log based metrices)
	c> Metrics Explorer: (set resource type to Executions and set some chars and see how it looks )
	d> Create charts on the Monitoring Overview window: (then switch to Monitoring overview there add the chart with the same configuration.)

46> Creating and Alerting on Logs-based Metrics:
	a> Create resources for the lab
	b> Monitor indexes
	c> Prepare to install Cloud Monitoring
	d> Additional resources for the lab (creating uptime check)
	e> System defined and user defined logs-based metrics
	f> User defined logs-based metrics
	g> Labels and user defined metrics
	h> Labels
	i> Create the Foodcount alerting policy
	j> Custom dashboard with heatmap

47> Deploy Your Website on Cloud Run: 
	a>Environment Setup (clone the file, run startup.sh and then npm start)
	b> Create Docker Container with Cloud Build (stores in cloud storage, makes dockerfile, runs to create image and pushes to cloud repository)
	c> Deploy Container To Cloud Run 
	d> Make Changes To The Website(Create new revision with lower concurrency)
	e> Update website with zero downtime
	f> Cleanup

48> Hosting a Web App on Google Cloud Using Compute Engine: (https://youtu.be/nnXi0ABwSXA)
	a> Environment Setup (Enabling compute engine api)
	b> Create Cloud Storage bucket
	c> Clone source repository(clone, cd, run setup.sh file, then run npm )
	d> Create Compute Engine instances: till now the compute engine api was hosting the app server. (creating startup script, copying the startup script to the created bucket, copying the cloned code to the bucket, deploying backend instance, 
	configuring connection to backend <editting .env file>, rebuilding the react app, copying code to bucket, Deploying frontend instance, configuring firewall rules for both the instances, )
	e> Create Managed Instance Groups(stop both the working instances, creating instance template for both frontend and backend, delete the backend instance as it has no use now, creating managed instance group for both frontend and backend, setting named ports for both frontend and backend
	configuring auto healing(creating health cheaks for both, creating firewall rule for both the health cheaks, applying the created healthcheaks to the respective services, ))
	f> Create Load Balancers (create load balancer for all 3 ports, 1 for frontend and 2 for backend, create backend services for all three ports, adding backend services to the 3 load balancers on 3 ports, creating a url map, creating a matcher, creating a proxy, creating a global forwarding rule)
	g> update configuration: (find ip of the load balancers, and update the .env file again, rebuild the react app, copy the application to a bucket, rolling action replaces the previous version with the newer version, )
	h> Scaling Compute Engine: (creating auto scaling poly for both frontend and backend, enabling content delievery network, (CDN))
	i> Update the website(changing machine type): (set the new machine type, create new instance template, roll out the updated instance, )
	j> Update the website (make changeds to the website): copy updated file to the old file (changed file), build the react app and upload the code to the cloud bucket, push out the rolling replacement.
	k> simulating failure: (enter into the ssh of any machine and kill the node, the auto healer will automatically heal the machine and keep the server up).


49> Deploy, Scale, and Update Your Website on Google Kubernetes Engine:
	a> Create a GKE cluster (enable container api and create the GKE cluster)
	b> Clone source repository and host it "awehi!"
	c> Create Docker container with Cloud Build: (Google Cloud Build will compress the files from the directory and move them to a Google Cloud Storage bucket. The build process will then take all the files from the bucket and use the Dockerfile to run the Docker build process. Since we specified the --tag flag with the host as gcr.io for the Docker image, the resulting Docker image will be pushed to the Google Cloud Container Registry.)
	d> Deploy container to GKE(with no downtime): (smallest unit a Gke can deploy is a pod a pod can contain single container or multiple containers)
	e> Expose GKE Deployment to the internet via the external ip.
	f> Scale GKE deployment from 1 pod to 3 
	g> Make changes to the website (into the index.js file, build the react app, and then use cloud build)
	h> Update website with zero downtime
	i> Cleanup

50> Migrating a Monolithic Website to Microservices on Google Kubernetes Engine: 
	a> Clone Source Repository
	b> Create a GKE Cluster
	c> Deploy Existing Monolith on one of the cluster's pod.
	d> Migrate Orders to a microservice: using seperate code base of the order service to create a new docker image and then deploy it on one of the pod on the old cluster.
	e> Migrate Products to Microservice: same as above
	f> Migrate frontend to microservice: same as above
	g> finally deleting the old monolith as we have completed deployment of the microservices.

51> Container optimized OS: Qwik start
	a> Create an instance using the console
	b> Create an instance using CLI
	
52> Datastore: Qwik Start: 

53> In this lab you will learn how to create and connect to a Google Cloud SQL MySQL instance and perform basic SQL operations using the Cloud Console and the mysql client.

54> Data Loss Prevention: Qwik Start - Command Line
	a> clone the github repo containing the DLP api
	b> install dependencies (npm install --save @google-cloud/dlp          npm install yargs)
	c> cd nodejs-dlp/samples
	d> code for inspecting a sample string:   node inspectString.js $DEVSHELL_PROJECT_ID "My email address is joe@example.com." LIKELY 0 EMAIL_ADDRESS DICT_TYPE true

55> 



















------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Terminologies: 
 The "Qwiklabs Resources" project contains files, datasets, and machine images for certain labs and can be accessed from every Google Cloud lab environment. It's important to note that "Qwiklabs Resources" is shared (read only) with all Qwiklabs users, which means that you cannot delete or modify it.

**> The Google Cloud project that you are working with is temporary, which means that the project and everything it contains will be deleted when the lab ends. Whenever you start a new lab, you will be given access to one or more new Google Cloud projects, and there (not "Qwiklabs Resources") is where you will run all of the lab steps.
1> resources that live in a zone are reffered to as zonal resources.
2> SSH or Secure Shell is a network communication protocol that enables two computers to communicate (c.f http or hypertext transfer protocol, which is the protocol used to transfer hypertext such as web pages) and share data.
3> Certain Compute Engine resources live in regions or zones. A region is a specific geographical location where you can run your resources. Each region has one or more zones. For example, the us-central1 region denotes a region in the Central United States that has zones us-central1-a, us-central1-b, us-central1-c, and us-central1-f.
4> Compute Engine lets you create and run virtual machines on Google infrastructure.
5> Google Kubernetes Engine (GKE) clusters are powered by the Kubernetes
6> Kubernetes an open source cluster management system.->  Kubernetes provides the mechanisms through which you interact with your container cluster. You use Kubernetes commands and resources to deploy and manage your applications, perform administrative tasks, set policies, and monitor the health of your deployed workloads.
7> When you run a GKE cluster, you also gain the benefit of advanced cluster management features that Google Cloud provides. These include:
 # Load balancing for Compute Engine instances
 # Node pools to designate subsets of nodes within a cluster for additional flexibility
 # Automatic scaling of your cluster's node instance count
 # Automatic upgrades for your cluster's node software
 # Node auto-repair to maintain node health and availability
 # Logging and Monitoring with Cloud Monitoring for visibility into your cluster
8> A cluster consists of at least one cluster master machine and multiple worker machines called nodes. Nodes are Compute Engine virtual machine (VM) instances that run the Kubernetes processes necessary to make them part of the cluster.  Note: Cluster names must start with a letter and end with an alphanumeric, and cannot be longer than 40 characters.
9> GKE uses Kubernetes objects to create and manage your cluster's resources. Kubernetes provides the Deployment object for deploying stateless applications like web servers. Service objects define rules and load balancing for accessing your application from the internet.
10> HTTP(S) Load Balancing is implemented on Google Front End (GFE). GFEs are distributed globally and operate together using Google's global network and control plane. You can configure URL rules to route some URLs to one set of instances and route other URLs to other instances. Requests are always routed to the instance group that is closest to the user, if that group has enough capacity and is appropriate for the request. If the closest group does not have enough capacity, the request is sent to the closest group that does have capacity.
11> fw-allow-health-check firewall rule. This is an ingress rule that allows traffic from the Google Cloud health checking systems (130.211.0.0/22 and 35.191.0.0/16). This lab uses the target tag allow-health-check to identify the VMs.
12> in http load balancing, there is 3 levels: url map > http proxy > forwarding rule.
13> ACL means access control list, used for giving access for buckets or its contents from google cloud shell.
14> Google Cloud's Identity and Access Management (IAM) service lets you create and manage permissions for Google Cloud resources. Cloud IAM unifies access control for Google Cloud services into a single system and provides a consistent set of operations. 
15> Cloud Monitoring provides visibility into the performance, uptime, and overall health of cloud-powered applications. Cloud Monitoring collects metrics, events, and metadata from Google Cloud, Amazon Web Services, hosted uptime probes, application instrumentation, and a variety of common application components including Cassandra, Nginx, Apache Web Server, Elasticsearch, and many others. Cloud Monitoring ingests that data and generates insights via dashboards, charts, and alerts. Cloud Monitoring alerting helps you collaborate by integrating with Slack, PagerDuty, HipChat, Campfire, and more.

16> Google Cloud Functions is a lightweight, event-based, asynchronous compute solution that allows you to create small, single-purpose functions that respond to cloud events without the need to manage a server or a runtime environment.
17> the common Cloud Functions use cases:
	a> Data processing/ETL ( ETL is a type of data integration that refers to the three steps (extract, transform, load) used to blend data from multiple sources. It's often used to build a data warehouse) 
	b> webhooks: Webhooks are one of a few ways web applications can communicate with each other. It allows you to send real-time data from one application to another whenever a given event occurs. 
	c> lightweight api's 
	d> Mobile backend 
	e> ioT 
18> When deploying a new function, you must specify --trigger-topic, --trigger-bucket, or --trigger-http. When deploying an update to an existing function, the function keeps the existing trigger unless otherwise specified. (in function deployment using the gcloud shell)

19> Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. A producer of data publishes messages to a Cloud Pub/Sub topic. A consumer creates a subscription to that topic. Subscribers either pull messages from a subscription or are configured as webhooks for push subscriptions. Every subscriber must acknowledge each message within a configurable window of time.
20> Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. By decoupling senders and receivers, it allows for secure and highly available communication between independently written applications. Google Cloud Pub/Sub delivers low-latency/durable messaging, and is commonly used by developers in implementing asynchronous workflows, distributing event notifications, and streaming data from various processes or devices.
21> A topic is a shared string that allows applications to connect with one another through a common thread. Publishers push (or publish) a message to a Cloud Pub/Sub topic. Subscribers make a "subscription" to a topic where they will either pull messages from the subscription or configure webhooks for push subscriptions. Every subscriber must acknowledge each message within a configurable window of time.

22> The Identity-Aware Proxy(Cloud IAP) controls access to your cloud applications and VMs running on Google Cloud Platform(GCP). <User Authentication: Identity-Aware Proxy>
23> Google Cloud VPCs let you increase the IP space of any subnets without any workload shutdown or downtime. This gives you flexibility and growth options to meet your needs.
24> The ICMP stands for (Internet Control Message Protocol). It is a network layer protocol. It is used for error handling in the network layer, and it is primarily used on network devices such as routers.
25> The Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.
26> Remote Desktop Protocol (RDP) is a proprietary protocol developed by Microsoft which provides a user with a graphical interface to connect to another computer over a network connection. 
27> VPC networks are by default isolated private networking domains. However, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC peering or VPN.
28> To test connectivity to mynet-eu-vm's internal IP, run the following command, replacing mynet-eu-vm's internal IP: " ping -c 3 <Enter mynet-eu-vm's internal IP here>" This does not work! In a multiple interface instance, every interface gets a route for the subnet that it is in. In addition, the instance gets a single default route that is associated with the primary interface eth0. Unless manually configured otherwise, any traffic leaving an instance for any destination other than a directly connected subnet will leave the instance via the default route on eth0. we can use external ip to communicate from any vpc to any other vpc, but by using internal ip we can only communicte to the vm in same vpc.
# Once an app is protected with IAP, it can use the identity information that IAP provides in the web request headers it passes through. In this step, the application will get the logged-in user's email address and a persistent unique user ID assigned by the Google Identity Service to that user. 

29> In VPC network controlling acess lab, we created 2 vm's with nginx installed on them and one more vm named test-vm(through command line). You are able to HTTP access both servers using their internal IP addresses. The connection on tcp:80 is allowed by the default-allow-internal firewall rule, as test-vm is on the same VPC network as the web servers default network).
30>  The following roles are used in conjunction with single-project networking to independently control administrative access to each VPC Network:
 # Network Admin: Permissions to create, modify, and delete networking resources, except for firewall rules and SSL certificates.
 # Security Admin: Permissions to create, modify, and delete firewall rules and SSL certificates.
31> The Compute Engine default service account does not have the right permissions to allow you to list or delete firewall rules. The same applies to other users who do not have the right roles. hence we create network or security admin roles, to alter the roles of vm's according to our need!

32> Health checks determine which instances of a load balancer can receive new connections. For HTTP load balancing, the health check probes to your load balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.
33> An instance template is an API resource that you use to create VM instances and managed instance groups. Instance templates define the machine type, boot disk image, subnet, labels, and other instance properties.
34> Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy and the autoscaler performs automatic scaling based on the measured load.
35> Backend services direct incoming traffic to one or more attached backends. Each backend is composed of an instance group and additional serving capacity metadata.

36> Google Cloud offers Internal Load Balancing for your TCP/UDP-based traffic. Internal Load Balancing enables you to run and scale your services behind a private load balancing IP address that is accessible only to your internal virtual machine instances.
37> Health checks determine which instances of a Load Balancer can receive new connections. For Internal load balancing, the health check probes to your load balanced instances come from addresses in the ranges 130.211.0.0/22 and 35.191.0.0/16. Your firewall rules must allow these connections.
38> The startup-script-url (under the management section while creating an instance template)specifies a script that will be executed when instances are started. This script installs Apache and changes the welcome page to include the client IP and the name, region and zone of the VM instance. Feel free to explore this script
39> Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in traffic and reduces cost when the need for resources is lower. You just define the autoscaling policy and the autoscaler performs automatic scaling based on the measured load.

40> Egress in the world of networking implies traffic that exits an entity or a network boundary, while Ingress is traffic that enters the boundary of a network. 

41> Docker is an open platform for developing, shipping, and running applications. With Docker, you can separate your applications from your infrastructure and treat your infrastructure like a managed application. Docker helps you ship code faster, test faster, deploy faster, and shorten the cycle between writing code and running code.
	Docker does this by combining kernel containerization features with workflows and tooling that helps you manage and deploy your applications.
	Docker containers can be directly used in Kubernetes, which allows them to be run in the Kubernetes Engine with ease. 
42> in docker the main concept is, you make a folder, make a Dockerfile which contains the instructions about the app.js file, which containes server code. now we create an image out of this two files, with a name and a tag associated to it. (these images can be listed by using <docker ps> command)then this image can be used to run in the form of containers 
43> To push images to your private registry hosted by gcr, you need to tag the images with a registry name. The format is [hostname]/[project-id]/[image]:[tag]

44> In kubernetes deployments keep the pods up and running even when the nodes they run on fail. In Kubernetes, all containers run in a pod.
45> Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single pod.
46> Pods can be created using pod configuration files. Pods also have Volumes. Volumes are data disks that live as long as the pods live, and can be used by the containers in that pod. Pods provide a shared namespace for their contents which means that the two containers inside of our example pod can communicate with each other, and they also share the attached volumes. Pods also share a network namespace. This means that there is one IP Address per pod.
47> Pods aren't meant to be persistent. They can be stopped or started for many reasons - like failed liveness or readiness checks - and this leads to a problem: What happens if you want to communicate with a set of Pods? When they get restarted they might have a different IP address. That's where Services come in. Services provide stable endpoints for Pods.
48> Services use labels to determine what Pods they operate on. If Pods have the correct labels, they are automatically picked up and exposed by our services.
The level of access a service provides to a set of pods depends on the Service's type. Currently there are three types:
 # ClusterIP (internal) -- the default type means that this Service is only visible inside of the cluster,
 # NodePort -- gives each node in the cluster an externally accessible IP and
 # LoadBalancer --  adds a load balancer from the cloud provider which forwards traffic from the service to Nodes within it.
49>  Deployments are a declarative way to ensure that the number of Pods running is equal to the desired number of Pods, specified by the user.
50> The main benefit of Deployments is in abstracting away the low level details of managing Pods. Behind the scenes Deployments use Replica Sets to manage starting and stopping the Pods. 

51> Different types of application deployments are as such: "Continuous Deployment", "Blue-Green Deployments", "Canary Deployments.
52> Heterogeneous deployments typically involve connecting two or more distinct infrastructure environments or regions to address a specific technical or operational need. Heterogeneous deployments are called "hybrid", "multi-cloud", or "public-private", depending upon the specifics of the deployment. For the purposes of this lab, heterogeneous deployments include those that span regions within a single cloud environment, multiple public cloud environments (multi-cloud), or a combination of on-premises and public cloud environments (hybrid or public-private).
Various business and technical challenges can arise in deployments that are limited to a single environment or region:
 # Maxed out resources: In any single environment, particularly in on-premises environments, you might not have the compute, networking, and storage resources to meet your production needs.
 # Limited geographic reach: Deployments in a single environment require people who are geographically distant from one another to access one deployment. Their traffic might travel around the world to a central location.
 # Limited availability: Web-scale traffic patterns challenge applications to remain fault-tolerant and resilient.
 # Vendor lock-in: Vendor-level platform and infrastructure abstractions can prevent you from porting applications.
 # Inflexible resources: Your resources might be limited to a particular set of compute, storage, or networking offerings.
53>  When you run the "kubectl create" <16.j> command to create the auth deployment, it will make one pod that conforms to the data in the Deployment manifest. This means we can scale the number of Pods by changing the number specified in the replicas field.
54> Deployments support updating images to a new version through a rolling update mechanism. When a Deployment is updated with a new version, it creates a new ReplicaSet and slowly increases the number of replicas in the new ReplicaSet as it decreases the replicas in the old ReplicaSet.
55> When you want to test a new deployment in production with a subset of your users, use a canary deployment. Canary deployments allow you to release a change to a small subset of your users to mitigate risk associated with new releases.
56> Rolling updates are ideal because they allow you to deploy an application slowly with minimal overhead, minimal performance impact, and minimal downtime. There are instances where it is beneficial to modify the load balancers to point to that new version only after it has been fully deployed. In this case, blue-green deployments are the way to go.
Kubernetes achieves this by creating two separate deployments; one for the old "blue" version and one for the new "green" version. Use your existing hello deployment for the "blue" version. The deployments will be accessed via a Service which will act as the router. Once the new "green" version is up and running, you'll switch over to using that version by updating the Service.
57> A major downside of blue-green deployments is that you will need to have at least 2x the resources in your cluster necessary to host your application. Make sure you have enough resources in your cluster before deploying both versions of the application at once.
	
58> Jenkins is an open-source automation server that lets you flexibly orchestrate your build, test, and deployment pipelines. Jenkins allows developers to iterate quickly on projects without worrying about overhead issues that can stem from continuous delivery.

59> SQL (Structured Query Language) is a standard language for data operations that allows you to ask questions and get insights from structured datasets. It's commonly used in database management and allows you to perform tasks like transaction record writing into relational databases and petabyte-scale data analysis.
60> BigQuery is a fully-managed petabyte-scale data warehouse that runs on the Google Cloud. Data analysts and data scientists can quickly query and filter large datasets, aggregate results, and perform complex operations without having to worry about setting up and managing servers.

61> A service is a grouping of pods that are running on the cluster. Services are "cheap" and you can have many services within the cluster. Kubernetes services can efficiently power a microservice architecture.
Services provide important features that are standardized across the cluster: load-balancing, service discovery between applications, and features to support zero-downtime application deployments.
Each service has a pod label query which defines the pods which will process data for the service. This label query frequently matches pods created by one or more replication controllers. Powerful routing scenarios are possible by updating a service's label query via the Kubernetes API with deployment software.
62> While you could use kubectl or similar CLI-based tools mapped to API calls to manage all Kubernetes resources described in YAML files, orchestration with Terraform presents a few benefits:
-You can use the same configuration language to provision the Kubernetes infrastructure and to deploy applications into it.
-Drift detection - terraform plan will always present you the difference between reality at a given time and config you intend to apply.
-Full lifecycle management - Terraform doesn't just initially create resources, but offers a single command to create, update, and delete tracked resources without needing to inspect the API to identify those resources.
-Synchronous feedback - While asynchronous behavior is often useful, sometimes it's counter-productive as the job of identifying operation result (failures or details of created resource) is left to the user. e.g. you don't have IP/hostname of load balancer until it has finished provisioning, hence you can't create any DNS record pointing to it.
-Graph of relationships - Terraform understands relationships between resources which may help in scheduling - e.g. Terraform won't try to create a service in a Kubernetes cluster until the cluster exists.

63> APM (application performance management)
64> service level indicators (SLIs), objectives (SLOs), and agreements (SLAs)

65> IAM Custom Roles: Note: You can create a custom role at the organization level and at the project level. However, you cannot create custom roles at the folder level.
There are two kinds of roles in Cloud IAM:
*Predefined Roles
*Custom Roles
66> You create a custom role by combining one or more of the available Cloud IAM permissions. Permissions allow users to perform specific actions on Google Cloud resources.
For example, the compute.instances.list permission allows a user to list the Compute Engine instances they own, while compute.instances.stop allows a user to stop a VM.
67> Permissions usually, but not always, correspond 1:1 with REST methods. That is, each Google Cloud service has an associated permission for each REST method that it has. To call a method, the caller needs that permission. For example, the caller of topic.publish() needs the pubsub.topics.publish permission.
68> Custom roles can only be used to grant permissions in policies for the same project or organization that owns the roles or resources under them. You cannot grant custom roles from one project or organization on a resource owned by a different project or organization.
69> Users who are not owners, including organization administrators, must be assigned either the "Organization Role Administrator role" (roles/iam.organizationRoleAdmin) or the IAM Role Administrator role (roles/iam.roleAdmin). 
* 70> The IAM Security Reviewer role (roles/iam.securityReviewer) enables the ability to view custom roles but not administer them.
71> The custom roles user interface is in the Cloud Console under IAM Roles. It is only available to users who have permissions to create or manage custom roles. By default, only project owners can create new roles. Project owners can control access to this feature by granting IAM Role Administrator role to others on the same project; for organizations, only Organization Administrators can grant the Organization Role, Administrator role.
72> Role metadata includes the role ID and permissions contained in the role. You can view the metadata using the Cloud Console or the IAM API. by using the command <gcloud iam roles describe [ROLE_NAME]>
73> To create a custom role, a caller must possess iam.roles.create permission. By default, the owner of a project or an organization has this permission and can create and manage custom roles.
74> we can use gcloud iam roles create command in two ways to create new custom roles: i.e 1> By providing a YAML file that contains the role definition	2> By using flags to specify the role definition When creating a custom role, you must specify whether it applies to the organization level or project level by using the --organization [ORGANIZATION_ID] or --project [PROJECT_ID] flags. Each example below creates a custom role at the project level.
75> updating custom roles is done using etags, for example: f two owners for a project try to make conflicting changes to a role at the same time, some changes could fail.hence, etag is used,  This property is used to verify if the custom role has changed since the last request. When you make a request to Cloud IAM with an etag value, Cloud IAM compares the etag value in the request with the existing etag value associated with the custom role. It writes the change only if the etag values match.
76> Use the gcloud iam roles update command to update custom roles. You can use this command in two ways:
By providing a YAML file that contains the updated role definition
By using flags to specify the updated role definition

77> Service accounts are a special type of Google account that grant permissions to virtual machines instead of end users. Service accounts are primarily used to ensure safe, managed connections to APIs and Google Cloud services. Granting access to trusted connections and rejecting malicious ones is a must-have security feature for any Google Cloud project.
78> A service account is a special Google account that belongs to your application or a virtual machine (VM) instead of an individual end user. Your application uses the service account to call the Google API of a service, so that the users aren't directly involved.
For example, a Compute Engine VM may run as a service account, and that account can be given permissions to access the resources it needs. This way the service account is the identity of the service, and the service account's permissions control which resources the service can access.
79> Different types of service accounts 
*User-managed service accounts
	When you create a new Cloud project using Cloud Console and if Compute Engine API is enabled for your project, a Compute Engine Service account is created for you by default. It is identifiable using the email: PROJECT_NUMBER-compute@developer.gserviceaccount.com
	If your project contains an App Engine application, the default App Engine service account is created in your project by default. It is identifiable using the email: PROJECT_ID@appspot.gserviceaccount.com
*Google-managed service accounts
	In addition to the user-managed service accounts, you might see some additional service accounts in your projectâ€™s IAM policy or in the Cloud Console. These service accounts are created and owned by Google. These accounts represent different Google services and each account is automatically granted IAM roles to access your Google Cloud project.
*Google APIs service account
	An example of a Google-managed service account is a Google API service account identifiable using the email:PROJECT_NUMBER@cloudservices.gserviceaccount.com This service account is designed specifically to run internal Google processes on your behalf and is not listed in the Service Accounts section of Cloud Console. By default, the account is automatically granted the project editor role on the project and is listed in the IAM section of Cloud Console. This service account is deleted only when the project is deleted. Google services rely on the account having access to your project, so you should not remove or change the service accountâ€™s role on your project.
80> When you create a new Cloud project, Google Cloud automatically creates one Compute Engine service account and one App Engine service account under that project. You can create up to 98 additional service accounts to your project to control access to your resources.
81> Creating a service account is similar to adding a member to your project, but the service account belongs to your applications rather than an individual end user.
82> When an identity calls a Google Cloud API, Google Cloud Identity and Access Management requires that the identity has the appropriate permissions to use the resource. You can grant permissions by granting roles to a user, a group, or a service account.
83> Types of Roles
There are three types of roles in Cloud IAM:
 *Primitive roles, which include the Owner, Editor, and Viewer roles that existed prior to the introduction of Cloud IAM.
 *Predefined roles, which provide granular access for a specific service and are managed by Google Cloud.
 *Custom roles, which provide granular access according to a user-specified list of permissions.
84> here in this lab we did the following:
	a) we created a virtual machine and gave the vm a door keeper i.e. the service account which contains the two permissions i.e.(Role: BigQuery Data Viewer and BigQuery User)
	b) now we opened the ssh of the vm created and installed python and its dependencies on it, then ran a querry which required some public databases acess
	c) here, comes the use of the service account. as the code on the ssh demands the access of the database the authorized service account serves as a passage for the vm to access the data base when needed!

85> VPC Network Peering allows you to build SaaS (Software-as-a-Service) ecosystems in Google Cloud, making services available privately across different VPC networks within and across organizations, allowing workloads to communicate in private space.
86> VPC Network Peering gives you several advantages over using external IP addresses or VPNs to connect networks, including:
 *Network Latency: Private networking offeres lower latency than public IP networking.
 *Network Security: Service owners do not need to have their services exposed to the public Internet and deal with its associated risks.
 *Network Cost: Networks that are peered can use internal IPs to communicate and save Google Cloud egress bandwidth costs. Regular network pricing still applies to all traffic.
87> Project names are unique across all of Google Cloud, so you do not need to specify the organization when setting up peering. Google Cloud knows the organization based on the project name.

-88> Cloud KMS is a REST API that can use a key to encrypt, decrypt, or sign data such as secrets for storage. High global availability. Cloud KMS is available in several global locations and across multi-regions, allowing you to place your service where you want for low latency and high availability
-89> In order to encrypt the data, you need to create a KeyRing and a CryptoKey. KeyRings are useful for grouping keys. Keys can be grouped by environment (like test, staging, and prod) or by some other conceptual grouping. For this lab, your KeyRing will be called test and your CryptoKey will be called qwiklab.
-90> Note: CryptoKeys and KeyRings cannot be deleted in Cloud KMS!
-91> In KMS, there are two major permissions to focus on. One permissions allows a user or service account to manage KMS resources, the other allows a user or service account to use keys to encrypt and decrypt data.
-The permission to manage keys is cloudkms.admin, and allows anyone with the permission to create KeyRings and create, modify, disable, and destroy CryptoKeys. The permission to encrypt and decrypt is cloudkms.cryptoKeyEncrypterDecrypter, and is used to call the encrypt and decrypt API endpoints.

-92> Only TCP, UDP and ICMP traffic may be mirrored. This, however, should satisfy the majority of use cases.
"Mirrored Sources" and "Collectors" must be in the SAME Region, but can be in different zones and even different VPCs, as long as those VPCs are properly Peered.
Additional bandwidth charges apply, especially between zones. To limit the traffic being mirrored, filters can be used.

93> In Kubernetes Engine, a private cluster is a cluster that makes your master inaccessible from the public internet. In a private cluster, nodes do not have public IP addresses, only private addresses, so your workloads run in an isolated environment. Nodes and masters communicate with each other using VPC peering.
94> When you create a private cluster, you must specify a /28 CIDR range (In the Kubernetes Engine API, address ranges are expressed as Classless Inter-Domain Routing (CIDR) blocks.) for the VMs that run the Kubernetes master components and you need to enable IP aliases.
95>  When you enable IP aliases, you let Kubernetes Engine automatically create a subnetwork for you.
96> in case of private cluster beta keyword is used 
97> In private cluster, the only IP addresses that have access to the master are the addresses in these ranges:
The primary range of your subnetwork. This is the range used for nodes.
The secondary range of your subnetwork that is used for pods.

98> All services need to exist as a part of a project and each project has a single billing account linked to it. though a billing account can have multiple associated projects and billing reports invoice will be able to breakdown cost by project.
99> Billing accounts can be directly linked to a google payments profile which is located outside of cloud, and it is used to pay for all google services like gsuits and google ads.
100> Billing accounts is set from the gcp console whereas for google payment profile visit: https://payments.google.com .
101> Understanding your gcp invoice : The bills can be paid via 2 options they are: 1> self serve or online (this is a default option while setting up a billing account, use debit or credit card for payment) 2> invoiced : 	(check or wired transfer and google assists with signup)
102> customers who are using the online metod for payment method receive a monthly statement whereas customers who have switched to invoice billing account receive a pdf invoice which is a request for payment
103> if you are looking for detailed gcp costs, reffer to the billing report on the gcp console 
104> exporting data to biGQuerry and creating a data studio to further explore it : these all actions can only be performed by billing administratiors

105> BigQuery is Google's serverless, highly scalable enterprise data warehouse that is designed to make data analysts more productive with unmatched price-performance.
106> Google Data Studio allows you to unlock the power of your data with interactive dashboards and beautiful reports that inspire smarter business decisions.
With Data Studio, you can:
 #Connect: easily access a wide variety of data. With built in and partner connectors, you can connect to virtually any type of data stream.
 #Visualize: turn your data into compelling stories of data visualization art. You can quickly build dashboards with Data Studio's web-based reporting tools.
 #Share: share your reports and dashboards with individuals, teams, or the world. Collaborate in real time. Embed your report on any page.

106> The Sheets data connector for BigQuery is available only to G Suite Business, Enterprise, and Education accounts.

107> The autoscaling application uses a Node.js script installed on Compute Engine instances. The script reports a numeric value to a Cloud monitoring metric. In response to the value of the metric, the application autoscales the Compute Engine instance group up or down as needed.
108> Compute Engine instance template - A template used to create each instance in the instance group.
 # Cloud Storage - A bucket used to host the startup script and other script files.
 # Compute Engine startup script - A startup script that installs the necessary code components on each instance. The startup script is installed and started automatically when an instance starts. When the startup script runs, it in turn installs and starts code on the instance that writes values to the Cloud monitoring custom metric.
 # Compute Engine instance group - An instance group that autoscales based on the Cloud monitoring metric values.
 # Compute Engine instances - A variable number of Compute Engine instances.
 # Custom Cloud Monitoring metric - A custom monitoring metric used as the input value for Compute Engine instance group autoscaling.

109> Helm is a package manager that makes it easy to configure and deploy Kubernetes applications. Your Cloud Shell will already have a recent, stable version of Helm pre-installed.

110> Memcached is one of the most popular open source, multi-purpose caching systems. It usually serves as a temporary store for frequently used data to speed up web applications and lighten database loads.
111> The Memcached Helm chart uses a StatefulSet controller. One benefit of using a StatefulSet controller is that the pods' names are ordered and predictable

** HTTP is a Hypertext Transfer Protocol, whereas TCP full form is Transmission Control Protocol. HTTP is utilized to access websites, while TCP is a session establishment protocol between client and server. HTTP uses port 80 and TCP uses no port. ... HTTP is faster in comparison to TCP, which is slower.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
112> BigQuery is a fully-managed, serverless data warehouse and a Big Data Analytics Platform, that enables scalable analysis over petabytes of data. It is a Platform as a Service that supports querying using ANSI SQL. It also has built-in machine learning capabilities.
113> You will encounter, for the first time, several aspects of Google Cloud that are of great benefit to scientists:
 # Serverless -- No need to download data to your machine in order to work with it - the dataset will remain on the cloud.
 # Ease of use -- Run ad-hoc SQL queries on your dataset without having to prepare the data, like indexes, beforehand. This is invaluable for data exploration.
 # Scale -- Carry out data exploration on extremely large datasets interactively. You don't need to sample the data in order to work with it in a timely manner.
 # Shareability -- You will be able to run queries on data from different datasets without any issues. BigQuery is a convenient way to share datasets. Of course, you can also keep your data private, or share them only with specific persons -- not all data need to be public.
   
114> AutoML Vision helps developers with limited ML expertise train high quality image recognition models. Once you upload images to the AutoML UI, you can train a model that will be immediately available on Google Cloud for generating predictions via an easy to use REST API.

115> The Places API is a service that returns information about points of interest by using HTTP requests. More specifically, you will take advantage of the Place Details and Place Photos services to receive detailed information and photos of establishments.

116> The App Engine standard environment is based on container instances running on Google's infrastructure. Containers are preconfigured with one of several available runtimes (Java 7, Java 8, Python 2.7, Go and PHP). Each runtime also includes libraries that support App Engine Standard APIs.
117> The App Engine standard environment makes it easy to build and deploy an application that runs reliably even under heavy load and with large amounts of data. It includes the following features:
 # Persistent storage with queries, sorting, and transactions.
 # Automatic scaling and load balancing.
 # Asynchronous task queues for performing work outside the scope of a request.
 # Scheduled tasks for triggering events at specified times or regular intervals.
 # Integration with other Google cloud services and APIs.
118> It is capable of building Web Application Archive (WAR) files for deployment into App Engine. Google provides a plugin and Maven Archetypes that are supported by Maven 3.5 or newer.

119> Dialogflow, which uses an underlying machine learning (ML) and natural language understanding (NLU) schema to build rich Assistant applications.
120> Action is interaction built for Google Assistant that performs specific tasks based on user input
121> Agent is A module that uses NLU and ML to transform user input into actionable data to be used by an Assistant application
122> Intent Takes user input and channels it to trigger an event.

123> APIs (Application Programming Interfaces) are software programs that give developers access to computing resources and data. Companies from many different fields offer publicly available APIs so that developers can integrate specialized tools, services, or libraries with their own applications and codebase.
124> Client server model - The internet is the standard communication channel that APIs use to transmit requests and responses between programs. The client-server model is the underlying architecture that web-based APIs use for exchanging information.
125> Since APIs use the web as a communication channel, many of them adhere to the HTTP protocol, which specifies rules and methods for data exchange between clients and servers over the internet. The HTTP protocol is not only used by APIs â€” it is the standard for web communication where data is sent and received over the internet.
126> Endpoints are access points to data or computing resources hosted on a server and they take the form of an HTTP URI. Endpoints are added to an API's base URL (e.g. http://example.com) to create a path to a specific resource or container of resources. The following are some examples of endpoints: http://example.com/storelocations
127> APIs that utilize the HTTP protocol, request methods, and endpoints are referred to as RESTful APIs. REST (Representational State Transfer) is an architectural style that prescribes standards for web-based communication.
128> JSON data is composed of key-value pairs. These are linked pieces of data that are composed of a unique identifier (a key) that references piece(s) of data (value). The key must be of type string and the value can be any of the data types (boolean, null, strings, arrays, Numbers).
129> Authentication refers to the process of determining a client's identity.                      Authorization refers to the process of determining what permissions an authenticated client has for a set of resources.
130> There are three types of authentication/authorization services that Google APIs use. These are "API Keys", "Service accounts", and "OAuth". An API will use one of these authentication services depending on the resources it requests and from where the API is called from.
 # API Keys : API keys are secret tokens that usually come in the form of an encrypted string. API keys are quick to generate and use. APIs that use public data or methods and want to get developers up and running quickly will oftentimes use API keys to authenticate users.
	In Google Cloud terms, API keys identify the calling project making the call to an API. By identifying the calling project, API keys enable usage information to be associated with that project, and they can reject calls from projects that haven't been granted access or enabled by the API.
 # OAuth: OAuth tokens are similar to API keys in their format, but they are more secure and can be linked to user accounts or identities. These tokens are used primarily when APIs give a developer the means to access user data.
	While API keys give developers access to all of an API's functionality, OAuth client IDs are all based on scope; different privileges will be granted to different identities.
 # Service Accounts: A service account is a special type of Google account that belongs to your application or a virtual machine (VM) instead of to an individual end user. Your application assumes the identity of the service account to call Google APIs, so that the users aren't directly involved.
	You can use a service account by providing its private key to your application, or by using the built-in service accounts available when running on Cloud Functions, Google App Engine, Compute Engine, or Google Kubernetes Engine.

131> Actions is the central platform for developing Google Assistant applications. The Actions platform integrates with human-computer interaction suites, which simplifies conversational app development. The most widely used suite is Dialogflow, which uses an underlying machine learning (ML) and natural language understanding (NLU) schema to build rich Assistant applications. The Actions platform also integrates with Cloud Functions, which lets you run backend fulfillment code in response to events triggered by Dialogflow requests
132> An agent is an organizational unit that collects information needed to complete a user's request, which it then forwards to a service that provides fulfillment logic.
133> When you created the Dialogflow agent, a Default Welcome intent is automatically created. This intent represents the main entry point into your conversation and the main action of your app. Your app must have a Default Welcome Intent defined, so that Actions on Google knows how to invoke your app
134> Entities extract specific data from user expressions and store them in accessible variables. By assigning this portion of the user says to be an entity, Dialogflow can extract parameters from the user input, validate the parameter and provide it to your fulfillment as a variable.
135> Slot filling also lets you set parameters as being required so that your agent doesn't process the input until the user provides all required parameters.
136> while creating a cloud function, index.js tab is open. This file defines your fulfillment logic and is used to create and deploy a Cloud Function
137> Now open the package.json tab. This file declares package dependencies for your fulfillment, including the Actions client library. 

138> We'll start with the Cloud Vision API's text detection method to make use of Optical Character Recognition (OCR) to extract text from images. Then we'll learn how to translate that text with the Translation API and analyze it with the Natural Language API.:   There are two ways to send an image to the Vision API for image detection: by sending the API a base64 encoded image string, or passing it the URL of a file stored in Cloud Storage. For this lab you'll create a Cloud Storage bucket to store your images.
139> curl is a command line tool to transfer data to or from a server, using any of the supported protocols (HTTP, FTP, IMAP, POP3, SCP, SFTP, SMTP, TFTP, TELNET, LDAP or FILE). curl is powered by Libcurl. This tool is preferred for automation, since it is designed to work without user interaction.

140> Cloud Vision API's text detection method to make use of Optical Character Recognition (OCR) to extract text from images.
142> The Translation API can translate text into 100+ languages. It can also detect the language of the input text. To translate the French text into English, all you need to do is pass the text and the language code for the target language (en-US) to the Translation API.
143> The Natural Language API helps us understand text by extracting entities, analyzing sentiment and syntax, and classifying text into categories. Use the analyzeEntities method to see what entities the Natural Language API can find in the text from your image.

144> Using the Natural Language API's classifyText method, you can sort text data into categories with a single API call. This method returns a list of content categories that apply to a text document. These categories range in specificity, from broad categories like /Computers & Electronics to highly specific categories such as /Computers & Electronics/Programming/Java (Programming Language). A full list of 700+ possible categories can be found here.
145> a service account is required when we are automating the process (scanning data from google cloud bucket, sending it to the nlp api, receiving and storing the data into a bigquerry table)
	
146> label detection.>>>  This method will return a list of labels (words) of what's in your image. it provides ("mid": "/m/01dk8s","description": "Powdered sugar", "Score": 0.9861496, "topicality": 0.9861496)
147> mid value that maps to the item's mid in Google's Knowledge Graph. You can use the mid when calling the Knowledge Graph API to get more information on the item.
148> webdetection >>> In addition to getting labels on what's in your image, the Vision API can also search the Internet for additional details on your image. Through the API's webDetection method, you get a lot of interesting data back:
A list of entities found in your image, based on content from pages with similar images
# URLs of exact and partial matching images found across the web, along with the URLs of those pages
# URLs of similar images, like doing a reverse image search
149> If you inpsect the URLs under fullMatchingImages, partialMatchingImages, and pagesWithMatchingImages, you'll notice that many of the URLs point to this lab site (super meta!).
150>face detection >>>the components received:
# boundingPoly gives you the x,y coordinates around the face in the image.
# fdBoundingPoly is a smaller box than boundingPoly, focusing on the skin part of the face.
# landmarks is an array of objects for each facial feature, some you may not have even known about. This tells us the type of landmark, along with the 3D position of that feature (x,y,z coordinates) where the z coordinate is the depth. The remaining values gives you more details on the face, including the likelihood of joy, sorrow, anger, and surprise.
151> landmark detection >>> attributes recieved:
 # the mid of the landmark
 # it's name (description)
 # a confidence score
 # The boundingPoly shows the region in the image where the landmark was identified.
 # The locations key tells us the latitude longitude coordinates of the picture.

152> The Cloud Natural Language API lets you extract entities from text, perform sentiment and syntactic analysis, and classify text into categories.
153> The first Natural Language API method you use is analyzeEntities. With this method, the API can extract entities (like people, places, and events) from text.
154> please go through the request file contents; as in case of NLP api and in case of Cloud Vision api in both case the contents sent are different.
155> IN the request.json for NLP api If you wanted to send a file from Cloud Storage instead of sending text, you would replace content with gcsContentUri and give it a value of the text file's uri in Cloud Storage.
156> For each entity in the response, you get the entity type, the associated Wikipedia URL if there is one, the salience, and the indices of where this entity appeared in the text. Salience is a number in the [0,1] range that refers to the centrality of the entity to the text as a whole
157> In addition to extracting entities, the Natural Language API also lets you perform sentiment analysis on a block of text.
158> The sentiment method returns two values:
 # score - is a number from -1.0 to 1.0 indicating how positive or negative the statement is.
 # magnitude - is a number ranging from 0 to infinity that represents the weight of sentiment expressed in the statement, regardless of being positive or negative.
 # Longer blocks of text with heavily weighted statements have higher magnitude values. The score for the first sentence is positive (0.7), whereas the score for the second sentence is neutral (0.1).
159> In addition to providing sentiment details on the entire text document, the Natural Language API can also break down sentiment by the entities in the text. Use this sentence as an example:
    I liked the sushi but the service was terrible.
    In this case, getting a sentiment score for the entire sentence as you did above might not be so useful. If this was a restaurant review and there were hundreds of reviews for the same restaurant, you'd want to know exactly which things people liked and didn't like in their reviews. Fortunately, the Natural Language API has a method that lets you get the sentiment for each entity in the text, called analyzeEntitySentiment. Let's see how it works!
160> Use syntactic analysis, another of the Natural Language API's method, to dive deeper into the the linguistic details of the text. analyzeSyntax extracts linguistic information, breaking up the given text into a series of sentences and tokens (generally, word boundaries), to provide further analysis on those tokens. For each word in the text, the API tells you the word's part of speech (noun, verb, adjective, etc.) and how it relates to other words in the sentence (Is it the root verb? A modifier?).
161> analyzeSyntax  
 # partOfSpeech tells you that "Joanne" is a noun.
 # dependencyEdge includes data that you can use to create a dependency parse tree of the text. Essentially, this is a diagram showing how words in a sentence relate to each other. 
 # headTokenIndex is the index of the token that has an arc pointing at "Joanne". Think of each token in the sentence as a word in an array.
 # headTokenIndex of 1 for "Joanne" refers to the word "Rowling", which it is connected to in the tree. The label NN (short for noun compound modifier) describes the word's role in the sentence. "Joanne" modifies "Rowling", the subject of the sentence.
 # lemma is the canonical form of the word. For example, the words run, runs, ran, and running all have a lemma of run. The lemma value is useful for tracking occurrences of a word in a large piece of text over time.
162> The Natural Language API also supports languages other than English. here you didn't tell the API which language the text is, it can automatically detect it! Next, you send it to the analyzeEntities endpoint

163> Python virtual environments are used to isolate package installation from the system.
164> The Awwvision lab uses Kubernetes and Cloud Vision API to demonstrate how to use the Vision API to classify (label) images from Reddit's /r/aww subreddit and display the labelled results in a web app. Awwvision has three components:
 # A simple Redis instance.
 # A web app that displays the labels and associated images.
 # A worker that handles scraping Reddit for images and classifying them using the Vision API. Cloud Pub/Sub is used to coordinate tasks between multiple worker instances.

165> Each project ID is unique across Google Cloud. Once you have created a project, you can delete the project but its ID can never be used again.
166> A project serves as a namespace. This means every resource within each project must have a unique name, but you can usually reuse resource names if they are in separate projects. Some resource names must be globally unique. Refer to the documentation for the resource for details.

167> Google Cloud Datastore is a NoSQL document database built for automatic scaling, high performance, and ease of application development. 
168> Cloud Datastore requires you to create an App Engine application in your project.

169> Cloud Storage allows world-wide storage and retrieval of any amount of data at any time. You can use Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download.

170> Google App Engine lets you manage resources from the command line, debug source code in production and run API backends. This lab concentrates on the backend service, putting together Pub/Sub, Natural Language, and Spanner services and APIs to collect and analyze feedback and scores from an online Quiz application.

171> Google Kubernetes Engine provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. The environment Kubernetes Engine provides consists of multiple machines (specifically, Compute Engine instances) grouped together to form a cluster.
Kubernetes provides the mechanisms through which you interact with your cluster. You use Kubernetes commands and resources to deploy and manage your applications, perform administration tasks and set policies, and monitor the health of your deployed workloads.

172> An App Engine app is a single application resource with one or more services. Each service can be configured to use different runtimes and to operate with different performance settings. Within each service, you can deploy versions of that service, and each then runs within one or more instances, depending on how much traffic you configured it to handle. For more information, see an overview of App Engine.
	App Engine uses either a Standard or Flexible environment. A standard environment runs instances in a sandbox, limiting available CPU options and disc access.
	In contrast, a flexible environment runs your application in Docker containers on Compute Engine virtual machines (VMs), which have fewer restrictions. For example, you can use the programming language or library of your choice, write to disk, and even run multiple processes. You also have the choice of Compute Engine machine types for your instances.

173> Cloud Profiler is a statistical, low-overhead profiler that continuously gathers CPU usage and memory-allocation information from your production applications. It attributes that information to the application's source code, helping you identify the parts of the application consuming the most resources, and otherwise illuminating the performance characteristics of the code.


174> Cloud Logging is part of the Operations suite of products in Google Cloud. It includes storage for logs, a user interface called the Logs Viewer, and an API to manage logs programmatically. Use Cloud Logging to read and write log entries, search and filter your logs, export your logs, and create logs-based metrics.
175> Log-based metrics are Cloud Monitoring metrics based on the content of log entries. Therefore, your logs don't just sit around and wait for someone to notice problems; Cloud Monitoring automatically monitors the logs for events and information you define in monitoring metrics. Log-based metrics are also a great way to achieve monitoring of your custom applications. If your application can write logs to a VM's filesystem, you can build monitoring on top of them!
	Cloud Logging provides two kinds of user-defined logs-based metrics - Counter and Distribution.
176> Counter metrics: Counter metrics count the number of log entries matching an advanced logs filter. For example, a metric that counts log entries representing certain types of errors from specific resources. Want to be alerted if a lot of your website visitors are receiving HTTP 500 errors? Counter metrics can help.
177> Distribution metrics: Distribution metrics accumulate numeric data from log entries matching a filter, and perform mathematical calculations against them. A common use for distribution metrics is to track latency patterns/trends over time. As each log entry is received, a latency value is extracted from the log entry and added to the distribution. At regular intervals, the accumulated distribution is written to Cloud Monitoring.
178> Cloud Logging retains logs for 30 days, after which they are deleted. To retain logs longer, you should export them to another storage system, or "sink", such as BigQuery. Cloud Logging allows you to set up automated exporting jobs so that all logs will automatically be exported. Logs may then be further analyzed with the features of your chosen sink.

179> Cloud Logging allows you to store, search, analyze, monitor, and alert on log data and events from the Google Cloud including BigQuery. Stackdriver also provides the ability to export certain logs to sinks such as Cloud Pub/Sub, Cloud Storage or BigQuery.

180> Cloud Logging can be used aggregate logs from all Google Cloud resources, as well as any custom resources on other platforms, to allow for one centralized store for all logs and metrics. Logs are aggregated and then viewable within the provided Cloud Logging UI. They can also be exported to Sinks to support more specialized of use cases. Currently, Cloud Logging supports exporting to the following sinks:
	# Cloud Storage
	# Pub/Sub
	# BigQuery
181> The Terraform configurations are going to build a Kubernetes Engine cluster that will generate logs and metrics that can be ingested by Stackdriver. The scripts will also build out Logging Export Sinks for Cloud Storage, BigQuery, and Cloud Pub/Sub
182> Following the principles of Infrastructure as Code and Immutable Infrastructure, Terraform supports the writing of declarative descriptions of the desired state of infrastructure. When the descriptor is applied, Terraform uses GCP APIs to provision and update resources to match. Terraform compares the desired state with the current state so incremental changes can be made without deleting everything and starting over. For instance, Terraform can build out GCP projects and compute instances, etc., even set up a Kubernetes Engine cluster and deploy applications to it. When requirements change, the descriptor can be updated and Terraform will adjust the cloud infrastructure accordingly.
183> By default, Kubernetes Engine clusters in GCP are provisioned with a pre-configured Fluentd-based collector that forwards logs to Cloud Logging. Interacting with the sample app will produce logs that are visible in the Cloud Logging and other log event sinks.
184> Log events can be stored in Cloud Storage, an object storage system suitable for archiving data. Policies can be configured for Cloud Storage buckets that, for instance, allow aging data to expire and be deleted while more recent data can be stored with a variety of storage classes affecting price and availability.
185> make teardown: Qwiklabs will take care of shutting down all the resources used for this lab, but hereâ€™s what you would need to do to clean up your own environment to save on cost and to be a good cloud citizen:

186> Logs-based metrics are Cloud Monitoring metrics that are based on the content of log entries. It can help you identify trends, extract numeric values out of the logs, and set up an alert when a certain log entry occurs by creating a metric for that event. You can use both system and user-defined logs-based metrics in Cloud Monitoring to create charts and alerting policies. Logs-based metrics are time series that are generated from data in logs
187> The Cloud Monitoring agent is a collectd-based daemon that gathers system and application metrics from virtual machine instances and sends them to Monitoring. By default, the Monitoring agent collects disk, CPU, network, and process metrics. Configuring the Monitoring agent allows third-party applications to get the full list of agent metrics.
188> The Cloud Logging agent streams logs from your VM instances and from selected third-party software packages to Cloud Logging. It is a best practice to run the Cloud Logging agent on all your VM instances
189> System defined logs-based metrics are ready to use right out of the box.These system logs-based metrics include:
Metrics around logs ingested
	# Byte_count: Number of bytes in all log entries ingested. This is broken down by monitored resource type, log stream name, and severity level.
Metrics around logs excluded
	# Excluded_byte_count: Number of bytes in log entries that were excluded. This is broken down by the monitored resource type.
	# Excluded_log_entry_count: Number of log entries that were excluded. This is broken down by the monitored resource type.
Metrics around logs based metrics
	# Dropped_log_entry_count: Despite the name, this does not show log entries dropped by Cloud logging but rather the number of log entries that did not contribute to logs based metrics because they arrived too late.
Log_entry_count: Number of log entries that contributed to logs based metrics so that dropped_log_entry_count + log_entry_count is the total number of log entries ingested by Cloud Logging.
	# Metric_throttled: Indicates if points are being dropped for logs-based metrics due to exceeding time series limits.
	# Time_series_count: Estimate of the active time series count for logs-based metrics.
190> Most system logs-based metrics are counter metrics. Counter metrics count the number of log entries that match an advanced logs filter.
191> Labels allow logs-based metrics to contain multiple time series â€” one for each label value. All logs-based metrics come with some default labels.
192> User defined labels can be created when you create a metric. An extractor expression is required for each configured label to tell Cloud Logging how to extract values from logs and place them as the labels' value. You cannot add labels to system logs-based metrics.

193> With Cloud Run, Google Cloud's implementation of Google's KNative framework, you can manage and deploy your website without any of the infrastructure overhead you experience with a VM or pure Kubernetes-based deployments. Not only is this a simpler approach from a management perspective, it also gives you the ability to "scale to zero" when there are no requests coming into your website.
194> Cloud Run brings "serverless" development to containers and can be run either on your own Google Kubernetes Engine (GKE) clusters or on a fully managed PaaS solution provided by Cloud Run.
195> Normally you would have to take a two step approach that entails building a docker container and pushing it to a registry to store the image for GKE to pull from. Make life easier and use Cloud Build to build the Docker container and put the image in Container Registry with a single command! 
196> Cloud Build will compress the files from the directory and move them to a Cloud Storage bucket. The build process will then take all the files from the bucket and use the Dockerfile, which is present in the same directory, to run the Docker build process. Since you specified the --tag flag with the host as gcr.io for the Docker image, the resulting Docker image will be pushed to Container Registry.
197> There are two approaches for deploying to Cloud Run:
	# Managed Cloud Run: The Platform as a Service model where all container lifecycle is managed by the Cloud Run product itself. You'll be using this approach in this lab.
	# Cloud Run on GKE: Cloud Run with an additional layer of control which allows you to bring your own clusters & pods from GKE. You can read more about it here.
198> You will deploy the image that was built earlier by the cloud build, and choose the managed version of *Cloud Run* by specifying --platform managed.
199> By default, a Cloud Run application will have a concurrency value of 80, meaning that each container instance will serve up to 80 requests at a time. This is a big departure from the Functions-as-a-Service model, where one instance handles one request at a time.
200> Cloud Run treats each deployment as a new Revision which will first be brought online, then have traffic redirected to it. By default the latest revision will be assigned 100% of the inbound traffic for a service. It is possible to use "Routes" to allocate different percentages of traffic to different revisions within a service.

201> There are many ways to deploy web sites within Google Cloud with each solution offering different features, capabilities, and levels of control. Compute Engine offers a deep level of control over the infrastructure used to run a web site, but also requires a little more operational management compared to solutions like Google Kubernetes Engines (GKE), App Engine, or others. With Compute Engine, you have fine-grained control of aspects of the infrastructure, including the virtual machines, load balancers, and more.
202> The deployment command and startup script is used with both the frontend and backend instances for simplicity, and because the code is configured to launch all microservices by default. As a result, all microservices run on both the frontend and backend in this sample. In a production environment you'd only run the microservices you need on each component.
203> To allow the application to scale, managed instance groups will be created and will use the frontend and backend instances as Instance Templates.
204> A managed instance group (MIG) contains identical instances that you can manage as a single entity in a single zone. Managed instance groups maintain high availability of your apps by proactively keeping your instances available, that is, in the RUNNING state. We will be using managed instance groups for our frontend and backend instances to provide autohealing, load balancing, autoscaling, and rolling updates.
205> Named ports can be assigned to an instance group, which indicates that the service is available on all instances in the group. This information is used by the HTTP Load Balancing service that will be configured later.
206> To improve the availability of the application itself and to verify it is responding, we configure an autohealing policy for the managed instance groups.
207> An autohealing policy relies on an application-based health check to verify that an app is responding as expected. Checking that an app responds is more precise than simply verifying that an instance is in a RUNNING state, which is the default behavior.
208>  Separate health checks for load balancing and for autohealing will be used. Health checks for load balancing can and should be more aggressive because these health checks determine whether an instance receives user traffic. You want to catch non-responsive instances quickly so you can redirect traffic if necessary. In contrast, health checking for autohealing causes Compute Engine to proactively replace failing instances, so this health check should be more conservative than a load balancing health check.
209> Create HTTP(S) Load Balancer
Google Cloud offers many different types of load balancers. For this lab you use an HTTP(S) Load Balancer for your traffic. An HTTP load balancer is structured as follows:
	# A forwarding rule directs incoming requests to a target HTTP proxy.
	# The target HTTP proxy checks each request against a URL map to determine the appropriate backend service for the request.
	# The backend service directs each request to an appropriate backend based on serving capacity, zone, and instance health of its attached backends. The health of each backend instance is verified using an HTTP health check. If the backend service is configured to use an HTTPS or HTTP/2 health check, the request will be encrypted on its way to the backend instance.
	# Sessions between the load balancer and the instance can use the HTTP, HTTPS, or HTTP/2 protocol. If you use HTTPS or HTTP/2, each instance in the backend services must have an SSL certificate.
210> In this example of a rolling replace, you specifically state that all machines can be replaced immediately through the --max-unavailable parameter. Without this parameter, the command would keep an instance alive while restarting others to ensure availability. For testing purposes, you specify to replace all immediately for speed.
211> When a user requests content from the HTTP(S) load balancer, the request arrives at a Google Front End (GFE) which first looks in the Cloud CDN cache for a response to the user's request. If the GFE finds a cached response, the GFE sends the cached response to the user. This is called a cache hit.
212> If the GFE can't find a cached response for the request, the GFE makes a request directly to the backend. If the response to this request is cacheable, the GFE stores the response in the Cloud CDN cache so that the cache can be used for subsequent requests.

213> Kubernetes represents applications as Pods, which are units that represent a container (or group of tightly-coupled containers). The Pod is the smallest deployable unit in Kubernetes. In this lab, each Pod contains only your monolith container.
214> To deploy your application, create a Deployment resource. The Deployment manages multiple copies of your application, called replicas, and schedules them to run on the individual nodes in your cluster. For this lab the Deployment will be running only one Pod of your application. Deployments ensure this by creating a ReplicaSet. The ReplicaSet is responsible for making sure the number of replicas specified are always running.
215> You have deployed your application on GKE, but you don't havthere isn't a way to access it outside of the cluster. By default, the containers you run on GKE are not accessible from the Internet because they do not have external IP addresses. You must explicitly expose your application to traffic from the Internet via a Service resource. A Service provides networking and IP support to your application's Pods. GKE creates an external IP and a Load Balancer for your application.
216> pods can be of a single container or multiple containers
217> GKE's rolling update mechanism ensures that your application remains up and available even as the system replaces instances of your old container image with your new one across all the running replicas.

218>  Breaking down an application into microservices has the following advantages, most of these stem from the fact that microservices are loosely coupled:
	# The microservices can be independently tested and deployed. The smaller the unit of deployment, the easier the deployment.
	# They can be implemented in different languages and frameworks. For each microservice, you're free to choose the best technology for its particular use case.
	# They can be managed by different teams. The boundary between microservices makes it easier to dedicate a team to one or several microservices.
	# By moving to microservices, you loosen the dependencies between the teams. Each team has to care only about the APIs of the microservices they are dependent on. The team doesn't need to think about how those microservices are implemented, about their release cycles, and so on.
 	# You can more easily design for failure. By having clear boundaries between services, it's easier to determine what to do if a service is down.
	# Some of the disadvantages when compared to monoliths are:

 	# Because a microservice-based app is a network of different services that often interact in ways that are not obvious, the overall complexity of the system tends to grow.
	# Unlike the internals of a monolith, microservices communicate over a network. In some circumstances, this can be seen as a security concern. Istio solves this problem by automatically encrypting the traffic between microservices.
	# It can be hard to achieve the same level of performance as with a monolithic approach because of latencies between services.
	# The behavior of your system isn't caused by a single service, but by many of them and by their interactions. Because of this, understanding how your system behaves in production (its observability) is harder. Istio is a solution to this problem as well.
219>  By default, the containers you run on GKE are not accessible from the Internet, because they do not have external IP addresses. You must explicitly expose your application to traffic from the Internet via a Service resource. A Service provides networking and IP support to your application's Pods. GKE creates an external IP and a Load Balancer ( subject to billing) for your application.

220>  install Cloud SDK to a virtual machine, initialize it and run core gcloud commands from the command-line. The Cloud SDK RPM packages are supported for Red Hat Enterprise Level 7 and CentOS 7.
221> The Cloud SDK RPM packages are supported for Red Hat Enterprise Level 7 and CentOS 7. They may also work on Fedora systems using yum or dnf, but this has not been tested.
222> gcloud init command in the ssh is used to perform several common SDK setup tasks. These include authorizing the SDK tools to access Google Cloud using your user account credentials and setting up the default SDK

223> Container-Optimized OS is an operating system image for your Compute Engine VMs that is optimized for running Docker containers, and is Google's recommended OS for running containers on Google Cloud. 
224> Since it comes with all container-related dependencies preinstalled, Container-Optimized OS allows your cluster to quickly scale up or down in response to traffic or workload changes, optimizing your spend and improving your reliability.
225> Container-Optimized OS powers many Google Cloud services such as Kubernetes Engine and Cloud SQL, making it Google's go-to solution for container workloads.
226> Run Containers Out of the Box: Container-Optimized OS instances come pre-installed with the Docker runtime and cloud-init. With a Container-Optimized OS instance, you can bring up your Docker container at the same time you create your VM, with no on-host setup required.
	# Smaller attack surface: Container-Optimized OS has a smaller footprint, reducing your instance's potential attack surface.
	# Locked-down by default: Container-Optimized OS instances include a locked-down firewall and other security settings by default.
	# Automatic Updates: Container-Optimized OS instances are configured to automatically download weekly updates in the background; only a reboot is necessary to use the latest updates.
227> Container-Optimized OS can be used to run most Docker containers. You should consider using Container-Optimized OS as the operating system for your Compute Engine instance if you have the following needs:
	# You need support for Docker containers or Kubernetes with minimal setup.
	# You need an operating system that has a small footprint and is security hardened for containers.
	# You need an operating system that is tested and verified for running Kubernetes on your Compute Engine instances.

228> The location applies to both Cloud Datastore and App Engine for your Google Cloud project. You cannot change the location after it has been saved.

229> The Data Loss Prevention API provides programmatic access to a powerful detection engine for personally identifiable information (PII) and other privacy-sensitive data in unstructured data streams.
230> The DLP API provides fast, scalable classification and optional redaction for sensitive data elements like credit card numbers, names, social security numbers, passport numbers, and phone numbers. The API supports text and images â€“ just send data to the API or specify data stored on your Cloud Storage, BigQuery, and Cloud Datastore instances.
